{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Stage 2: Vector Database Population\n",
    "\n",
    "This notebook is the second stage of the Elyra pipeline. It handles:\n",
    "1. Loading processed chunks from the previous stage\n",
    "2. Creating embeddings using local models\n",
    "3. Populating LanceDB vector database\n",
    "4. Creating searchable indexes\n",
    "5. Preparing data for graph database integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for vector processing\n",
    "!pip install lancedb\n",
    "!pip install sentence-transformers\n",
    "!pip install pyarrow\n",
    "!pip install pandas\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "print(f\"Vector processing started at: {datetime.now()}\")\n",
    "print(\"‚úÖ All required packages imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Previous Stage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from previous pipeline stage\n",
    "INPUT_DIR = Path(\"pipeline_outputs\")\n",
    "VECTOR_DB_DIR = Path(\"vector_db\")\n",
    "VECTOR_DB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load vector-ready chunks\n",
    "vector_chunks_file = INPUT_DIR / \"vector_ready_chunks.json\"\n",
    "with open(vector_chunks_file, 'r') as f:\n",
    "    vector_chunks = json.load(f)\n",
    "\n",
    "# Load pipeline metadata\n",
    "metadata_file = INPUT_DIR / \"pipeline_metadata.json\"\n",
    "with open(metadata_file, 'r') as f:\n",
    "    pipeline_metadata = json.load(f)\n",
    "\n",
    "print(f\"üì• Loaded {len(vector_chunks)} chunks from previous stage\")\n",
    "print(f\"üìÑ Source documents: {pipeline_metadata['successful_documents']}\")\n",
    "print(f\"üìä Total tokens: {pipeline_metadata['total_tokens']:,}\")\n",
    "\n",
    "# Show sample chunk\n",
    "if vector_chunks:\n",
    "    print(f\"\\nüìã Sample chunk structure:\")\n",
    "    sample = vector_chunks[0]\n",
    "    print(f\"  ID: {sample['id']}\")\n",
    "    print(f\"  Content length: {len(sample['content'])} chars\")\n",
    "    print(f\"  Document type: {sample['metadata']['document_type']}\")\n",
    "    print(f\"  Element path: {sample['metadata']['element_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence transformer for embeddings\n",
    "# Using a lightweight model that works well for technical documents\n",
    "model_name = \"all-MiniLM-L6-v2\"  # 384 dimensions, good performance/size ratio\n",
    "\n",
    "print(f\"ü§ñ Loading embedding model: {model_name}\")\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Test the model\n",
    "test_embedding = embedding_model.encode(\"This is a test sentence.\")\n",
    "print(f\"‚úÖ Model loaded successfully\")\n",
    "print(f\"üìè Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"üéØ Model max sequence length: {embedding_model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_batch(chunks: List[Dict], batch_size: int = 32) -> List[Dict]:\n",
    "    \"\"\"Create embeddings for chunks in batches for efficiency\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Creating embeddings for {len(chunks)} chunks...\")\n",
    "    \n",
    "    enriched_chunks = []\n",
    "    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        batch_texts = [chunk['content'] for chunk in batch]\n",
    "        \n",
    "        # Create embeddings for batch\n",
    "        batch_embeddings = embedding_model.encode(\n",
    "            batch_texts, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Add embeddings to chunks\n",
    "        for j, chunk in enumerate(batch):\n",
    "            enriched_chunk = {\n",
    "                **chunk,\n",
    "                'vector': batch_embeddings[j].tolist(),  # Convert to list for JSON serialization\n",
    "                'embedding_model': model_name,\n",
    "                'vector_created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            enriched_chunks.append(enriched_chunk)\n",
    "        \n",
    "        print(f\"  ‚úÖ Processed batch {i//batch_size + 1}/{(len(chunks) + batch_size - 1)//batch_size}\")\n",
    "    \n",
    "    return enriched_chunks\n",
    "\n",
    "# Create embeddings for all chunks\n",
    "enriched_chunks = create_embeddings_batch(vector_chunks)\n",
    "print(f\"\\nüéâ Created embeddings for {len(enriched_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LanceDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LanceDB\n",
    "db_path = VECTOR_DB_DIR / \"xml_analysis_db\"\n",
    "db = lancedb.connect(str(db_path))\n",
    "\n",
    "print(f\"üóÉÔ∏è Connected to LanceDB at: {db_path}\")\n",
    "\n",
    "# Prepare data for LanceDB (requires specific format)\n",
    "def prepare_lancedb_data(chunks: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert enriched chunks to LanceDB format\"\"\"\n",
    "    \n",
    "    lancedb_data = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Flatten metadata for easier querying\n",
    "        metadata = chunk['metadata']\n",
    "        \n",
    "        record = {\n",
    "            'id': chunk['id'],\n",
    "            'content': chunk['content'],\n",
    "            'vector': chunk['vector'],\n",
    "            \n",
    "            # Document metadata\n",
    "            'source_file': metadata['source_file'],\n",
    "            'document_type': metadata['document_type'],\n",
    "            'confidence': metadata['confidence'],\n",
    "            \n",
    "            # Chunk metadata\n",
    "            'chunk_id': metadata['chunk_id'],\n",
    "            'element_path': metadata['element_path'],\n",
    "            'token_estimate': metadata['token_estimate'],\n",
    "            \n",
    "            # AI use cases (convert list to string for simplicity)\n",
    "            'ai_use_cases': ', '.join(metadata['ai_use_cases']),\n",
    "            \n",
    "            # Processing metadata\n",
    "            'embedding_model': chunk['embedding_model'],\n",
    "            'vector_created_at': chunk['vector_created_at']\n",
    "        }\n",
    "        \n",
    "        lancedb_data.append(record)\n",
    "    \n",
    "    return lancedb_data\n",
    "\n",
    "# Prepare data\n",
    "lancedb_data = prepare_lancedb_data(enriched_chunks)\n",
    "print(f\"üìä Prepared {len(lancedb_data)} records for LanceDB\")\n",
    "\n",
    "# Show sample record structure\n",
    "if lancedb_data:\n",
    "    sample_record = lancedb_data[0]\n",
    "    print(f\"\\nüìã Sample record keys: {list(sample_record.keys())}\")\n",
    "    print(f\"üî¢ Vector dimension: {len(sample_record['vector'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table in LanceDB\n",
    "table_name = \"xml_documents\"\n",
    "\n",
    "# Drop table if it exists (for testing)\n",
    "try:\n",
    "    db.drop_table(table_name)\n",
    "    print(f\"üóëÔ∏è Dropped existing table: {table_name}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new table with data\n",
    "print(f\"üìù Creating table: {table_name}\")\n",
    "table = db.create_table(table_name, lancedb_data)\n",
    "\n",
    "print(f\"‚úÖ Table created with {len(lancedb_data)} records\")\n",
    "print(f\"üìä Table schema: {table.schema}\")\n",
    "\n",
    "# Create vector index for faster similarity search\n",
    "print(f\"üîç Creating vector index...\")\n",
    "table.create_index(\"vector\")\n",
    "print(f\"‚úÖ Vector index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vector_search(query: str, limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Test vector similarity search\"\"\"\n",
    "    \n",
    "    # Create query embedding\n",
    "    query_vector = embedding_model.encode(query)\n",
    "    \n",
    "    # Search for similar chunks\n",
    "    results = table.search(query_vector).limit(limit).to_pandas()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test searches\n",
    "test_queries = [\n",
    "    \"geographic coordinates and location data\",\n",
    "    \"security vulnerabilities and compliance\",\n",
    "    \"XML schema and document structure\"\n",
    "]\n",
    "\n",
    "print(f\"üîç Testing vector search functionality:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüîé Query: '{query}'\")\n",
    "    results = test_vector_search(query, limit=3)\n",
    "    \n",
    "    if len(results) > 0:\n",
    "        print(f\"üìä Found {len(results)} results:\")\n",
    "        for i, row in results.iterrows():\n",
    "            print(f\"  {i+1}. {row['source_file']} | {row['document_type']} | Score: {row['_distance']:.3f}\")\n",
    "            print(f\"     Content: {row['content'][:100]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå No results found\")\n",
    "\n",
    "print(f\"\\n‚úÖ Vector search testing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Graph Database Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_graph_relationships(chunks: List[Dict]) -> Dict[str, List]:\n",
    "    \"\"\"Extract relationships for graph database population\"\"\"\n",
    "    \n",
    "    nodes = []\n",
    "    relationships = []\n",
    "    \n",
    "    # Group chunks by document\n",
    "    docs_by_file = {}\n",
    "    for chunk in chunks:\n",
    "        file_name = chunk['metadata']['source_file']\n",
    "        if file_name not in docs_by_file:\n",
    "            docs_by_file[file_name] = []\n",
    "        docs_by_file[file_name].append(chunk)\n",
    "    \n",
    "    # Create document nodes and chunk relationships\n",
    "    for file_name, file_chunks in docs_by_file.items():\n",
    "        if not file_chunks:\n",
    "            continue\n",
    "            \n",
    "        # Document node\n",
    "        doc_metadata = file_chunks[0]['metadata']\n",
    "        doc_node = {\n",
    "            'id': f\"doc_{file_name}\",\n",
    "            'type': 'Document',\n",
    "            'properties': {\n",
    "                'name': file_name,\n",
    "                'document_type': doc_metadata['document_type'],\n",
    "                'confidence': doc_metadata['confidence'],\n",
    "                'ai_use_cases': doc_metadata['ai_use_cases'],\n",
    "                'total_chunks': len(file_chunks)\n",
    "            }\n",
    "        }\n",
    "        nodes.append(doc_node)\n",
    "        \n",
    "        # Chunk nodes and relationships\n",
    "        for chunk in file_chunks:\n",
    "            chunk_node = {\n",
    "                'id': chunk['id'],\n",
    "                'type': 'Chunk',\n",
    "                'properties': {\n",
    "                    'chunk_id': chunk['metadata']['chunk_id'],\n",
    "                    'element_path': chunk['metadata']['element_path'],\n",
    "                    'token_estimate': chunk['metadata']['token_estimate'],\n",
    "                    'content_length': len(chunk['content'])\n",
    "                }\n",
    "            }\n",
    "            nodes.append(chunk_node)\n",
    "            \n",
    "            # Document contains chunk relationship\n",
    "            relationship = {\n",
    "                'from': doc_node['id'],\n",
    "                'to': chunk['id'],\n",
    "                'type': 'CONTAINS',\n",
    "                'properties': {\n",
    "                    'element_path': chunk['metadata']['element_path']\n",
    "                }\n",
    "            }\n",
    "            relationships.append(relationship)\n",
    "    \n",
    "    # Create document type similarity relationships\n",
    "    doc_types = {}\n",
    "    for node in nodes:\n",
    "        if node['type'] == 'Document':\n",
    "            doc_type = node['properties']['document_type']\n",
    "            if doc_type not in doc_types:\n",
    "                doc_types[doc_type] = []\n",
    "            doc_types[doc_type].append(node)\n",
    "    \n",
    "    # Add similarity relationships between documents of same type\n",
    "    for doc_type, docs in doc_types.items():\n",
    "        if len(docs) > 1:\n",
    "            for i in range(len(docs)):\n",
    "                for j in range(i + 1, len(docs)):\n",
    "                    relationship = {\n",
    "                        'from': docs[i]['id'],\n",
    "                        'to': docs[j]['id'],\n",
    "                        'type': 'SIMILAR_TYPE',\n",
    "                        'properties': {\n",
    "                            'document_type': doc_type\n",
    "                        }\n",
    "                    }\n",
    "                    relationships.append(relationship)\n",
    "    \n",
    "    return {\n",
    "        'nodes': nodes,\n",
    "        'relationships': relationships\n",
    "    }\n",
    "\n",
    "# Extract graph data\n",
    "graph_data = extract_graph_relationships(enriched_chunks)\n",
    "\n",
    "print(f\"üï∏Ô∏è Graph database preparation:\")\n",
    "print(f\"  üìä Nodes: {len(graph_data['nodes'])}\")\n",
    "print(f\"  üîó Relationships: {len(graph_data['relationships'])}\")\n",
    "\n",
    "# Show node type distribution\n",
    "node_types = {}\n",
    "for node in graph_data['nodes']:\n",
    "    node_type = node['type']\n",
    "    node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "\n",
    "print(f\"\\nüìã Node types:\")\n",
    "for node_type, count in node_types.items():\n",
    "    print(f\"  ‚Ä¢ {node_type}: {count}\")\n",
    "\n",
    "# Show relationship type distribution\n",
    "rel_types = {}\n",
    "for rel in graph_data['relationships']:\n",
    "    rel_type = rel['type']\n",
    "    rel_types[rel_type] = rel_types.get(rel_type, 0) + 1\n",
    "\n",
    "print(f\"\\nüîó Relationship types:\")\n",
    "for rel_type, count in rel_types.items():\n",
    "    print(f\"  ‚Ä¢ {rel_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pipeline Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enriched chunks with embeddings\n",
    "enriched_chunks_file = INPUT_DIR / \"enriched_chunks_with_vectors.json\"\n",
    "with open(enriched_chunks_file, 'w') as f:\n",
    "    json.dump(enriched_chunks, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Saved enriched chunks to: {enriched_chunks_file}\")\n",
    "\n",
    "# Save graph data for next stage\n",
    "graph_data_file = INPUT_DIR / \"graph_data.json\"\n",
    "with open(graph_data_file, 'w') as f:\n",
    "    json.dump(graph_data, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Saved graph data to: {graph_data_file}\")\n",
    "\n",
    "# Save vector database info\n",
    "vector_db_info = {\n",
    "    'database_path': str(db_path),\n",
    "    'table_name': table_name,\n",
    "    'total_records': len(lancedb_data),\n",
    "    'embedding_model': model_name,\n",
    "    'vector_dimension': len(lancedb_data[0]['vector']) if lancedb_data else 0,\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'schema': str(table.schema) if 'table' in locals() else None\n",
    "}\n",
    "\n",
    "vector_db_info_file = INPUT_DIR / \"vector_db_info.json\"\n",
    "with open(vector_db_info_file, 'w') as f:\n",
    "    json.dump(vector_db_info, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Saved vector DB info to: {vector_db_info_file}\")\n",
    "\n",
    "# Update pipeline metadata\n",
    "pipeline_metadata.update({\n",
    "    'vector_processing': {\n",
    "        'processed_at': datetime.now().isoformat(),\n",
    "        'embedding_model': model_name,\n",
    "        'vector_dimension': len(lancedb_data[0]['vector']) if lancedb_data else 0,\n",
    "        'total_vectors': len(enriched_chunks),\n",
    "        'database_path': str(db_path),\n",
    "        'table_name': table_name\n",
    "    },\n",
    "    'graph_preparation': {\n",
    "        'total_nodes': len(graph_data['nodes']),\n",
    "        'total_relationships': len(graph_data['relationships']),\n",
    "        'node_types': node_types,\n",
    "        'relationship_types': rel_types\n",
    "    }\n",
    "})\n",
    "\n",
    "updated_metadata_file = INPUT_DIR / \"pipeline_metadata_updated.json\"\n",
    "with open(updated_metadata_file, 'w') as f:\n",
    "    json.dump(pipeline_metadata, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Updated pipeline metadata: {updated_metadata_file}\")\n",
    "\n",
    "print(f\"\\nüéâ Vector database population stage completed successfully!\")\n",
    "print(f\"üìä Vector database contains {len(lancedb_data)} searchable documents\")\n",
    "print(f\"üï∏Ô∏è Graph data prepared with {len(graph_data['nodes'])} nodes\")\n",
    "print(f\"Ready for next pipeline stage: Graph Database Population & RAG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}