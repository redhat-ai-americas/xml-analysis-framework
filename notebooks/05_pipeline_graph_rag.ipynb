{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Stage 3: Graph Database & RAG System\n",
    "\n",
    "This notebook is the final stage of the Elyra pipeline. It handles:\n",
    "1. Loading vector and graph data from previous stages\n",
    "2. Populating Memgraph database with relationships\n",
    "3. Creating a unified RAG system combining vector and graph search\n",
    "4. Demonstrating intelligent document Q&A\n",
    "5. Generating insights and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for graph and RAG processing\n",
    "%pip install pymgclient --upgrade -q --force-reinstall --no-cache-dir\n",
    "%pip install lancedb --upgrade -q --force-reinstall --no-cache-dir\n",
    "%pip install sentence-transformers --upgrade -q --force-reinstall --no-cache-dir\n",
    "%pip install pandas --upgrade -q --force-reinstall --no-cache-dir\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import mgclient\n",
    "\n",
    "print(f\"Graph and RAG processing started at: {datetime.now()}\")\n",
    "print(\"✅ All required packages imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Previous Stage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from previous pipeline stages\n",
    "INPUT_DIR = Path(\"pipeline_outputs\")\n",
    "VECTOR_DB_DIR = Path(\"vector_db\")\n",
    "\n",
    "# Load graph data\n",
    "graph_data_file = INPUT_DIR / \"graph_data.json\"\n",
    "with open(graph_data_file, 'r') as f:\n",
    "    graph_data = json.load(f)\n",
    "\n",
    "# Load vector database info\n",
    "vector_db_info_file = INPUT_DIR / \"vector_db_info.json\"\n",
    "with open(vector_db_info_file, 'r') as f:\n",
    "    vector_db_info = json.load(f)\n",
    "\n",
    "# Load updated pipeline metadata\n",
    "metadata_file = INPUT_DIR / \"pipeline_metadata_updated.json\"\n",
    "with open(metadata_file, 'r') as f:\n",
    "    pipeline_metadata = json.load(f)\n",
    "\n",
    "print(f\"📊 Loaded graph data: {len(graph_data['nodes'])} nodes, {len(graph_data['relationships'])} relationships\")\n",
    "print(f\"🗃️ Vector database: {vector_db_info['total_records']} records\")\n",
    "print(f\"📄 Source documents: {pipeline_metadata['successful_documents']}\")\n",
    "\n",
    "# Reconnect to vector database\n",
    "db_path = vector_db_info['database_path']\n",
    "table_name = vector_db_info['table_name']\n",
    "db = lancedb.connect(db_path)\n",
    "vector_table = db.open_table(table_name)\n",
    "\n",
    "print(f\"✅ Reconnected to vector database: {table_name}\")\n",
    "\n",
    "# Reload embedding model\n",
    "model_name = vector_db_info['embedding_model']\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "print(f\"🤖 Reloaded embedding model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Memgraph (Simulated)\n",
    "\n",
    "**Note**: This assumes you have Memgraph running locally. If not, the code will simulate the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemgraphConnector:\n",
    "    \"\"\"Wrapper for Memgraph operations with fallback simulation\"\"\"\n",
    "    \n",
    "    def __init__(self, host=\"localhost\", port=7687):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.connection = None\n",
    "        self.simulated = False\n",
    "        \n",
    "        try:\n",
    "            self.connection = mgclient.connect(host=host, port=port)\n",
    "            print(f\"✅ Connected to Memgraph at {host}:{port}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not connect to Memgraph: {e}\")\n",
    "            print(f\"🔄 Running in simulation mode\")\n",
    "            self.simulated = True\n",
    "            self.simulated_data = {'nodes': [], 'relationships': []}\n",
    "    \n",
    "    def execute_query(self, query: str, parameters: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Execute Cypher query\"\"\"\n",
    "        if self.simulated:\n",
    "            print(f\"🔄 [SIMULATED] Query: {query[:100]}...\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(query, parameters or {})\n",
    "            results = cursor.fetchall()\n",
    "            return [dict(record) for record in results]\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Query failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_node(self, node: Dict) -> bool:\n",
    "        \"\"\"Create a node in the graph\"\"\"\n",
    "        query = f\"\"\"\n",
    "        CREATE (n:{node['type']} {{\n",
    "            id: $id,\n",
    "            {', '.join([f'{k}: ${k}' for k in node['properties'].keys()])}\n",
    "        }})\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {'id': node['id'], **node['properties']}\n",
    "        \n",
    "        if self.simulated:\n",
    "            self.simulated_data['nodes'].append(node)\n",
    "            return True\n",
    "        \n",
    "        result = self.execute_query(query, parameters)\n",
    "        return result is not None\n",
    "    \n",
    "    def create_relationship(self, rel: Dict) -> bool:\n",
    "        \"\"\"Create a relationship in the graph\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MATCH (a), (b)\n",
    "        WHERE a.id = $from_id AND b.id = $to_id\n",
    "        CREATE (a)-[r:{rel['type']} {{\n",
    "            {', '.join([f'{k}: ${k}' for k in rel['properties'].keys()])}\n",
    "        }}]->(b)\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'from_id': rel['from'],\n",
    "            'to_id': rel['to'],\n",
    "            **rel['properties']\n",
    "        }\n",
    "        \n",
    "        if self.simulated:\n",
    "            self.simulated_data['relationships'].append(rel)\n",
    "            return True\n",
    "        \n",
    "        result = self.execute_query(query, parameters)\n",
    "        return result is not None\n",
    "    \n",
    "    def clear_database(self):\n",
    "        \"\"\"Clear all data from the database\"\"\"\n",
    "        if self.simulated:\n",
    "            self.simulated_data = {'nodes': [], 'relationships': []}\n",
    "            print(\"🔄 [SIMULATED] Database cleared\")\n",
    "            return\n",
    "        \n",
    "        self.execute_query(\"MATCH (n) DETACH DELETE n\")\n",
    "        print(\"🗑️ Database cleared\")\n",
    "\n",
    "# Initialize graph database connection\n",
    "graph_db = MemgraphConnector()\n",
    "\n",
    "# Clear database for clean start\n",
    "graph_db.clear_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_graph_database(graph_db: MemgraphConnector, graph_data: Dict) -> Dict[str, int]:\n",
    "    \"\"\"Populate the graph database with nodes and relationships\"\"\"\n",
    "    \n",
    "    stats = {'nodes_created': 0, 'relationships_created': 0, 'errors': 0}\n",
    "    \n",
    "    # Create nodes\n",
    "    print(f\"📝 Creating {len(graph_data['nodes'])} nodes...\")\n",
    "    for i, node in enumerate(graph_data['nodes']):\n",
    "        try:\n",
    "            success = graph_db.create_node(node)\n",
    "            if success:\n",
    "                stats['nodes_created'] += 1\n",
    "            else:\n",
    "                stats['errors'] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating node {node['id']}: {e}\")\n",
    "            stats['errors'] += 1\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  ✅ Created {i + 1}/{len(graph_data['nodes'])} nodes\")\n",
    "    \n",
    "    # Create relationships\n",
    "    print(f\"\\n🔗 Creating {len(graph_data['relationships'])} relationships...\")\n",
    "    for i, rel in enumerate(graph_data['relationships']):\n",
    "        try:\n",
    "            success = graph_db.create_relationship(rel)\n",
    "            if success:\n",
    "                stats['relationships_created'] += 1\n",
    "            else:\n",
    "                stats['errors'] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating relationship {rel['from']}->{rel['to']}: {e}\")\n",
    "            stats['errors'] += 1\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  ✅ Created {i + 1}/{len(graph_data['relationships'])} relationships\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Populate the database\n",
    "population_stats = populate_graph_database(graph_db, graph_data)\n",
    "\n",
    "print(f\"\\n📊 Graph database population completed:\")\n",
    "print(f\"  ✅ Nodes created: {population_stats['nodes_created']}\")\n",
    "print(f\"  ✅ Relationships created: {population_stats['relationships_created']}\")\n",
    "print(f\"  ❌ Errors: {population_stats['errors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Unified RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedRAGSystem:\n",
    "    \"\"\"RAG system combining vector similarity and graph relationships\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_table, graph_db, embedding_model):\n",
    "        self.vector_table = vector_table\n",
    "        self.graph_db = graph_db\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def vector_search(self, query: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Perform vector similarity search\"\"\"\n",
    "        query_vector = self.embedding_model.encode(query)\n",
    "        results = self.vector_table.search(query_vector).limit(limit).to_pandas()\n",
    "        return results.to_dict('records')\n",
    "    \n",
    "    def graph_search(self, document_type: str = None, relationship_type: str = None) -> List[Dict]:\n",
    "        \"\"\"Perform graph-based search\"\"\"\n",
    "        if self.graph_db.simulated:\n",
    "            # Simulate graph search results\n",
    "            simulated_results = []\n",
    "            for node in self.graph_db.simulated_data['nodes']:\n",
    "                if document_type and node['type'] == 'Document':\n",
    "                    if node['properties'].get('document_type') == document_type:\n",
    "                        simulated_results.append(node)\n",
    "                elif not document_type:\n",
    "                    simulated_results.append(node)\n",
    "            return simulated_results[:5]  # Limit results\n",
    "        \n",
    "        # Real graph query\n",
    "        query = \"MATCH (n) \"\n",
    "        conditions = []\n",
    "        \n",
    "        if document_type:\n",
    "            conditions.append(f\"n.document_type = '{document_type}'\")\n",
    "        \n",
    "        if conditions:\n",
    "            query += \"WHERE \" + \" AND \".join(conditions)\n",
    "        \n",
    "        query += \" RETURN n LIMIT 5\"\n",
    "        \n",
    "        return self.graph_db.execute_query(query)\n",
    "    \n",
    "    def find_related_documents(self, document_id: str) -> List[Dict]:\n",
    "        \"\"\"Find documents related to a given document\"\"\"\n",
    "        if self.graph_db.simulated:\n",
    "            # Simulate related document search\n",
    "            related = []\n",
    "            for rel in self.graph_db.simulated_data['relationships']:\n",
    "                if rel['from'] == document_id or rel['to'] == document_id:\n",
    "                    other_id = rel['to'] if rel['from'] == document_id else rel['from']\n",
    "                    for node in self.graph_db.simulated_data['nodes']:\n",
    "                        if node['id'] == other_id:\n",
    "                            related.append(node)\n",
    "                            break\n",
    "            return related[:3]\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        MATCH (d1)-[r]-(d2)\n",
    "        WHERE d1.id = '{document_id}' AND d2.type = 'Document'\n",
    "        RETURN d2, r.type as relationship_type\n",
    "        LIMIT 3\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.graph_db.execute_query(query)\n",
    "    \n",
    "    def unified_search(self, query: str, context: Dict = None) -> Dict[str, Any]:\n",
    "        \"\"\"Perform unified search combining vector and graph approaches\"\"\"\n",
    "        \n",
    "        # Vector search for content similarity\n",
    "        vector_results = self.vector_search(query, limit=3)\n",
    "        \n",
    "        # Extract document types from vector results for graph search\n",
    "        doc_types = list(set([r.get('document_type') for r in vector_results if r.get('document_type')]))\n",
    "        \n",
    "        # Graph search for structural relationships\n",
    "        graph_results = []\n",
    "        for doc_type in doc_types[:2]:  # Limit to 2 document types\n",
    "            graph_results.extend(self.graph_search(document_type=doc_type))\n",
    "        \n",
    "        # Find related documents\n",
    "        related_docs = []\n",
    "        if vector_results:\n",
    "            # Use the top vector result to find related documents\n",
    "            top_result = vector_results[0]\n",
    "            source_file = top_result.get('source_file', '')\n",
    "            doc_id = f\"doc_{source_file}\"\n",
    "            related_docs = self.find_related_documents(doc_id)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'vector_results': vector_results,\n",
    "            'graph_results': graph_results,\n",
    "            'related_documents': related_docs,\n",
    "            'search_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def answer_question(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate an answer to a question using RAG approach\"\"\"\n",
    "        \n",
    "        # Perform unified search\n",
    "        search_results = self.unified_search(question)\n",
    "        \n",
    "        # Extract relevant content\n",
    "        contexts = []\n",
    "        sources = []\n",
    "        \n",
    "        for result in search_results['vector_results']:\n",
    "            contexts.append(result.get('content', ''))\n",
    "            sources.append({\n",
    "                'file': result.get('source_file', ''),\n",
    "                'document_type': result.get('document_type', ''),\n",
    "                'chunk_id': result.get('chunk_id', ''),\n",
    "                'similarity_score': result.get('_distance', 0.0)\n",
    "            })\n",
    "        \n",
    "        # Generate answer (simulated - in real implementation, use LLM)\n",
    "        answer = self._generate_simulated_answer(question, contexts, search_results)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'related_documents': len(search_results['related_documents']),\n",
    "            'confidence': self._calculate_confidence(search_results)\n",
    "        }\n",
    "    \n",
    "    def _generate_simulated_answer(self, question: str, contexts: List[str], search_results: Dict) -> str:\n",
    "        \"\"\"Generate a simulated answer (replace with actual LLM in production)\"\"\"\n",
    "        \n",
    "        if not contexts:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "        \n",
    "        # Analyze question type\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        if \"what is\" in question_lower or \"what are\" in question_lower:\n",
    "            doc_types = [r.get('document_type', '') for r in search_results['vector_results']]\n",
    "            unique_types = list(set(doc_types))\n",
    "            \n",
    "            answer = f\"Based on the analyzed documents, I found information related to {', '.join(unique_types)} documents. \"\n",
    "            answer += f\"The most relevant content includes: {contexts[0][:200]}...\"\n",
    "            \n",
    "        elif \"how many\" in question_lower or \"count\" in question_lower:\n",
    "            answer = f\"I found {len(search_results['vector_results'])} relevant document chunks. \"\n",
    "            answer += f\"These span across {len(search_results['related_documents'])} related documents.\"\n",
    "            \n",
    "        elif \"where\" in question_lower or \"location\" in question_lower:\n",
    "            sources = [r.get('source_file', '') for r in search_results['vector_results']]\n",
    "            answer = f\"The information can be found in: {', '.join(set(sources))}. \"\n",
    "            answer += \"These documents contain geographic or location-related data.\"\n",
    "            \n",
    "        else:\n",
    "            answer = f\"Based on my analysis of the document collection, here's what I found: \"\n",
    "            answer += f\"The most relevant information comes from {len(search_results['vector_results'])} document sections. \"\n",
    "            if contexts:\n",
    "                answer += f\"Key content: {contexts[0][:150]}...\"\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def _calculate_confidence(self, search_results: Dict) -> float:\n",
    "        \"\"\"Calculate confidence score for the answer\"\"\"\n",
    "        vector_results = search_results['vector_results']\n",
    "        \n",
    "        if not vector_results:\n",
    "            return 0.0\n",
    "        \n",
    "        # Base confidence on similarity scores and number of results\n",
    "        similarity_scores = [1 - r.get('_distance', 1.0) for r in vector_results]\n",
    "        avg_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0\n",
    "        \n",
    "        # Boost confidence if we have related documents\n",
    "        related_boost = min(0.1, len(search_results['related_documents']) * 0.05)\n",
    "        \n",
    "        return min(1.0, avg_similarity + related_boost)\n",
    "\n",
    "# Initialize the unified RAG system\n",
    "rag_system = UnifiedRAGSystem(vector_table, graph_db, embedding_model)\n",
    "print(\"🤖 Unified RAG system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system with various questions\n",
    "test_questions = [\n",
    "    \"What geographic information is available in the documents?\",\n",
    "    \"How many document types were analyzed?\",\n",
    "    \"What are the main AI use cases for these documents?\",\n",
    "    \"Where can I find location or coordinate data?\",\n",
    "    \"What XML schemas are represented in the collection?\"\n",
    "]\n",
    "\n",
    "print(\"🤖 Testing RAG System:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rag_results = []\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n❓ Question {i}: {question}\")\n",
    "    \n",
    "    # Get answer from RAG system\n",
    "    result = rag_system.answer_question(question)\n",
    "    rag_results.append(result)\n",
    "    \n",
    "    print(f\"💡 Answer: {result['answer']}\")\n",
    "    print(f\"📊 Confidence: {result['confidence']:.2f}\")\n",
    "    print(f\"📚 Sources: {len(result['sources'])} document chunks\")\n",
    "    \n",
    "    if result['sources']:\n",
    "        print(f\"📁 Top source: {result['sources'][0]['file']} ({result['sources'][0]['document_type']})\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\n✅ RAG system testing completed: {len(rag_results)} questions processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_collection_insights(rag_system: UnifiedRAGSystem, pipeline_metadata: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"Generate insights about the entire document collection\"\"\"\n",
    "    \n",
    "    insights = {\n",
    "        'collection_overview': {\n",
    "            'total_documents': pipeline_metadata['successful_documents'],\n",
    "            'total_chunks': pipeline_metadata['total_chunks'],\n",
    "            'total_tokens': pipeline_metadata['total_tokens'],\n",
    "            'processing_date': pipeline_metadata['vector_processing']['processed_at']\n",
    "        },\n",
    "        'document_types': {},\n",
    "        'ai_applications': set(),\n",
    "        'technical_recommendations': [],\n",
    "        'search_performance': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze document types through vector search\n",
    "    type_queries = [\n",
    "        \"geographic data coordinates\",\n",
    "        \"security configuration\",\n",
    "        \"XML schema structure\",\n",
    "        \"data format specification\"\n",
    "    ]\n",
    "    \n",
    "    for query in type_queries:\n",
    "        results = rag_system.vector_search(query, limit=5)\n",
    "        for result in results:\n",
    "            doc_type = result.get('document_type', 'Unknown')\n",
    "            if doc_type not in insights['document_types']:\n",
    "                insights['document_types'][doc_type] = {\n",
    "                    'count': 0,\n",
    "                    'ai_use_cases': set(),\n",
    "                    'avg_confidence': 0.0\n",
    "                }\n",
    "            \n",
    "            insights['document_types'][doc_type]['count'] += 1\n",
    "            \n",
    "            # Extract AI use cases\n",
    "            use_cases = result.get('ai_use_cases', '').split(', ')\n",
    "            for use_case in use_cases:\n",
    "                if use_case.strip():\n",
    "                    insights['document_types'][doc_type]['ai_use_cases'].add(use_case.strip())\n",
    "                    insights['ai_applications'].add(use_case.strip())\n",
    "    \n",
    "    # Convert sets to lists for JSON serialization\n",
    "    for doc_type in insights['document_types']:\n",
    "        insights['document_types'][doc_type]['ai_use_cases'] = list(\n",
    "            insights['document_types'][doc_type]['ai_use_cases']\n",
    "        )\n",
    "    \n",
    "    insights['ai_applications'] = list(insights['ai_applications'])\n",
    "    \n",
    "    # Generate recommendations\n",
    "    insights['technical_recommendations'] = [\n",
    "        {\n",
    "            'category': 'Vector Search Optimization',\n",
    "            'recommendation': 'Consider using domain-specific embedding models for better semantic understanding',\n",
    "            'priority': 'Medium',\n",
    "            'rationale': f'Current model handles {len(insights[\"ai_applications\"])} different AI use cases'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Graph Relationships',\n",
    "            'recommendation': 'Expand graph schema to include semantic similarity relationships',\n",
    "            'priority': 'High',\n",
    "            'rationale': 'Would improve cross-document discovery and related content suggestions'\n",
    "        },\n",
    "        {\n",
    "            'category': 'RAG Enhancement',\n",
    "            'recommendation': 'Implement query expansion using document type ontologies',\n",
    "            'priority': 'Medium',\n",
    "            'rationale': f'Could improve recall across {len(insights[\"document_types\"])} document types'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Analyze search performance\n",
    "    performance_queries = [\n",
    "        \"geographic coordinates\",\n",
    "        \"security settings\",\n",
    "        \"XML structure\"\n",
    "    ]\n",
    "    \n",
    "    performance_scores = []\n",
    "    for query in performance_queries:\n",
    "        results = rag_system.vector_search(query, limit=1)\n",
    "        if results:\n",
    "            score = 1 - results[0].get('_distance', 1.0)\n",
    "            performance_scores.append(score)\n",
    "    \n",
    "    insights['search_performance'] = {\n",
    "        'avg_similarity_score': sum(performance_scores) / len(performance_scores) if performance_scores else 0,\n",
    "        'test_queries': len(performance_queries),\n",
    "        'vector_dimension': pipeline_metadata['vector_processing']['vector_dimension']\n",
    "    }\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generate comprehensive insights\n",
    "print(\"📊 Generating collection insights...\")\n",
    "collection_insights = generate_collection_insights(rag_system, pipeline_metadata)\n",
    "\n",
    "print(f\"\\n🎯 Collection Insights Summary:\")\n",
    "print(f\"📄 Documents: {collection_insights['collection_overview']['total_documents']}\")\n",
    "print(f\"📝 Chunks: {collection_insights['collection_overview']['total_chunks']:,}\")\n",
    "print(f\"🔤 Tokens: {collection_insights['collection_overview']['total_tokens']:,}\")\n",
    "\n",
    "print(f\"\\n📋 Document Types:\")\n",
    "for doc_type, info in collection_insights['document_types'].items():\n",
    "    print(f\"  • {doc_type}: {info['count']} occurrences\")\n",
    "    print(f\"    AI use cases: {', '.join(info['ai_use_cases'][:3])}\")\n",
    "\n",
    "print(f\"\\n🤖 AI Applications ({len(collection_insights['ai_applications'])}):\")\n",
    "for app in collection_insights['ai_applications'][:5]:\n",
    "    print(f\"  • {app}\")\n",
    "\n",
    "print(f\"\\n💡 Top Recommendations:\")\n",
    "for rec in collection_insights['technical_recommendations']:\n",
    "    print(f\"  • {rec['category']}: {rec['recommendation']}\")\n",
    "    print(f\"    Priority: {rec['priority']} | {rec['rationale']}\")\n",
    "\n",
    "print(f\"\\n⚡ Search Performance:\")\n",
    "perf = collection_insights['search_performance']\n",
    "print(f\"  • Average similarity: {perf['avg_similarity_score']:.3f}\")\n",
    "print(f\"  • Vector dimension: {perf['vector_dimension']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RAG test results\n",
    "rag_results_file = INPUT_DIR / \"rag_test_results.json\"\n",
    "with open(rag_results_file, 'w') as f:\n",
    "    json.dump(rag_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"💾 Saved RAG test results to: {rag_results_file}\")\n",
    "\n",
    "# Save collection insights\n",
    "insights_file = INPUT_DIR / \"collection_insights.json\"\n",
    "with open(insights_file, 'w') as f:\n",
    "    json.dump(collection_insights, f, indent=2, default=str)\n",
    "\n",
    "print(f\"💾 Saved collection insights to: {insights_file}\")\n",
    "\n",
    "# Save graph database stats\n",
    "graph_stats = {\n",
    "    'population_stats': population_stats,\n",
    "    'simulated_mode': graph_db.simulated,\n",
    "    'total_nodes': len(graph_data['nodes']),\n",
    "    'total_relationships': len(graph_data['relationships']),\n",
    "    'node_types': pipeline_metadata['graph_preparation']['node_types'],\n",
    "    'relationship_types': pipeline_metadata['graph_preparation']['relationship_types']\n",
    "}\n",
    "\n",
    "graph_stats_file = INPUT_DIR / \"graph_database_stats.json\"\n",
    "with open(graph_stats_file, 'w') as f:\n",
    "    json.dump(graph_stats, f, indent=2)\n",
    "\n",
    "print(f\"💾 Saved graph database stats to: {graph_stats_file}\")\n",
    "\n",
    "# Create final pipeline summary\n",
    "final_summary = {\n",
    "    'pipeline_completed_at': datetime.now().isoformat(),\n",
    "    'stages_completed': ['data_ingestion', 'vector_population', 'graph_rag'],\n",
    "    'input_files': pipeline_metadata['input_files'],\n",
    "    'processing_summary': {\n",
    "        'documents_processed': pipeline_metadata['successful_documents'],\n",
    "        'chunks_created': pipeline_metadata['total_chunks'],\n",
    "        'vectors_generated': pipeline_metadata['vector_processing']['total_vectors'],\n",
    "        'graph_nodes': graph_stats['total_nodes'],\n",
    "        'graph_relationships': graph_stats['total_relationships']\n",
    "    },\n",
    "    'rag_system': {\n",
    "        'questions_tested': len(rag_results),\n",
    "        'avg_confidence': sum(r['confidence'] for r in rag_results) / len(rag_results) if rag_results else 0,\n",
    "        'search_performance': collection_insights['search_performance']\n",
    "    },\n",
    "    'recommendations': collection_insights['technical_recommendations'],\n",
    "    'output_files': {\n",
    "        'vector_database': vector_db_info['database_path'],\n",
    "        'rag_results': str(rag_results_file),\n",
    "        'insights': str(insights_file),\n",
    "        'graph_stats': str(graph_stats_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "final_summary_file = INPUT_DIR / \"pipeline_final_summary.json\"\n",
    "with open(final_summary_file, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"💾 Saved final pipeline summary to: {final_summary_file}\")\n",
    "\n",
    "print(f\"\\n🎉 Complete pipeline execution finished successfully!\")\n",
    "print(f\"📊 Processed {final_summary['processing_summary']['documents_processed']} documents\")\n",
    "print(f\"🔍 Created searchable vector database with {final_summary['processing_summary']['vectors_generated']} embeddings\")\n",
    "print(f\"🕸️ Built knowledge graph with {final_summary['processing_summary']['graph_nodes']} nodes\")\n",
    "print(f\"🤖 Tested RAG system with {final_summary['rag_system']['questions_tested']} questions\")\n",
    "print(f\"💡 Generated {len(final_summary['recommendations'])} improvement recommendations\")\n",
    "\n",
    "print(f\"\\n📁 All outputs saved to: {INPUT_DIR}\")\n",
    "print(f\"✨ Ready for production deployment and further development!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
