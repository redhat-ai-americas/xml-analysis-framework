{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real AI Agent Integration with XML Analysis Framework\n",
    "\n",
    "This notebook demonstrates **actual** AI agent integration using LangChain and the XML Analysis Framework. Everything shown here uses real API calls and generates actual AI responses.\n",
    "\n",
    "**Key Distinctions:**\n",
    "- 🤖 **AI-Generated**: Content created by actual LLM API calls (clearly marked)\n",
    "- 📊 **Framework-Prepared**: Data structured by our framework for AI consumption (not AI-generated)\n",
    "\n",
    "**Requirements:**\n",
    "- OpenAI API key (configured in .env file)\n",
    "- Internet connection for API calls\n",
    "- All packages installed from the setup cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "XML Analysis Framework version: 1.2.12\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install xml-analysis-framework==1.2.12 --upgrade -q --force-reinstall --no-cache-dir\n",
    "%pip install langchain --upgrade -q --force-reinstall --no-cache-dir\n",
    "%pip install langchain-openai --upgrade -q --force-reinstall --no-cache-dir\n",
    "%pip install langchain-community --upgrade -q --force-reinstall --no-cache-dir\n",
    "%pip install python-dotenv --upgrade -q --force-reinstall --no-cache-dir\n",
    "\n",
    "import xml_analysis_framework as xaf\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"XML Analysis Framework version: {xaf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real AI Setup (LangChain + OpenAI)\n",
    "\n",
    "This section sets up actual AI components that will make real API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API key loaded: sk-proj-...\n",
      "✅ OpenAI LLM initialized!\n",
      "📊 Using model: gpt-4o\n",
      "📏 Context window: Unknown\n",
      "🌡️ Temperature: 0.1\n",
      "📝 Max output tokens: 4096\n",
      "✅ Memory buffer initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rk/l45jnpxn3g1b9865j26zbbmr0000gn/T/ipykernel_1285/1362014527.py:115: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# Configure model selection and setup\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This loads variables from .env file\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain_openai import OpenAI, ChatOpenAI  # For OpenAI models\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "# ========================================\n",
    "# MODEL SELECTION - CHANGE THIS SECTION\n",
    "# ========================================\n",
    "\n",
    "# Option 1: OpenAI Models (current)\n",
    "MODEL_PROVIDER = \"openai\"  # Options: \"openai\", \"anthropic\", \"local\"\n",
    "\n",
    "# OpenAI Configuration\n",
    "OPENAI_CONFIG = {\n",
    "    \"model_name\": \"gpt-4o\",  # Options: \"gpt-4\", \"gpt-4-turbo-preview\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 4096,\n",
    "    \"use_chat_model\": True,  # Use ChatOpenAI for better chat-based interactions\n",
    "}\n",
    "\n",
    "# Model Context Windows (for reference)\n",
    "CONTEXT_WINDOWS = {\n",
    "    # OpenAI\n",
    "    \"gpt-4\": \"8k tokens\",\n",
    "    \"gpt-4-turbo-preview\": \"128k tokens\",\n",
    "    \"gpt-3.5-turbo\": \"4k tokens\", \n",
    "    \"gpt-3.5-turbo-16k\": \"16k tokens\",\n",
    "    # Anthropic\n",
    "    \"claude-3-opus-20240229\": \"200k tokens\",\n",
    "    \"claude-3-sonnet-20240229\": \"200k tokens\",\n",
    "    \"claude-3-haiku-20240307\": \"200k tokens\",\n",
    "    # Local/Open models\n",
    "    \"llama-2-70b\": \"4k tokens\",\n",
    "    \"mixtral-8x7b\": \"32k tokens\",\n",
    "    \"deepseek-coder\": \"16k tokens\"\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# Initialize based on provider\n",
    "# ========================================\n",
    "\n",
    "llm = None\n",
    "\n",
    "if MODEL_PROVIDER == \"openai\":\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if openai_api_key:\n",
    "        print(f\"✅ OpenAI API key loaded: {openai_api_key[:8]}...\")\n",
    "        \n",
    "        if OPENAI_CONFIG[\"use_chat_model\"]:\n",
    "            # Use ChatOpenAI for better chat interactions\n",
    "            llm = ChatOpenAI(\n",
    "                model_name=OPENAI_CONFIG[\"model_name\"],\n",
    "                temperature=OPENAI_CONFIG[\"temperature\"],\n",
    "                max_tokens=OPENAI_CONFIG[\"max_tokens\"],\n",
    "                openai_api_key=openai_api_key\n",
    "            )\n",
    "        else:\n",
    "            # Use standard OpenAI\n",
    "            llm = OpenAI(\n",
    "                model_name=OPENAI_CONFIG[\"model_name\"],\n",
    "                temperature=OPENAI_CONFIG[\"temperature\"],\n",
    "                max_tokens=OPENAI_CONFIG[\"max_tokens\"],\n",
    "                openai_api_key=openai_api_key\n",
    "            )\n",
    "        \n",
    "        print(f\"✅ OpenAI LLM initialized!\")\n",
    "        print(f\"📊 Using model: {OPENAI_CONFIG['model_name']}\")\n",
    "        print(f\"📏 Context window: {CONTEXT_WINDOWS.get(OPENAI_CONFIG['model_name'], 'Unknown')}\")\n",
    "        print(f\"🌡️ Temperature: {OPENAI_CONFIG['temperature']}\")\n",
    "        print(f\"📝 Max output tokens: {OPENAI_CONFIG['max_tokens']}\")\n",
    "    else:\n",
    "        print(\"⚠️ OPENAI_API_KEY not found. Please check your .env file.\")\n",
    "\n",
    "elif MODEL_PROVIDER == \"anthropic\":\n",
    "    # Example for Anthropic Claude (requires: pip install langchain-anthropic)\n",
    "    print(\"ℹ️ To use Anthropic Claude:\")\n",
    "    print(\"1. Install: pip install langchain-anthropic\")\n",
    "    print(\"2. Add to .env: ANTHROPIC_API_KEY=your_key\")\n",
    "    print(\"3. Update imports and initialization:\")\n",
    "    print(\"\"\"\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=4096,\n",
    "    anthropic_api_key=anthropic_api_key\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "elif MODEL_PROVIDER == \"local\":\n",
    "    # Example for local models (Ollama, llama.cpp, etc.)\n",
    "    print(\"ℹ️ To use local models:\")\n",
    "    print(\"1. Install: pip install langchain-community\")\n",
    "    print(\"2. Run local model server (e.g., Ollama)\")\n",
    "    print(\"3. Update initialization:\")\n",
    "    print(\"\"\"\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama2:70b\",  # or \"mixtral\", \"deepseek-coder\", etc.\n",
    "    temperature=0.1\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Initialize memory (works with any LLM)\n",
    "if llm:\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    print(\"✅ Memory buffer initialized!\")\n",
    "else:\n",
    "    print(\"❌ No LLM initialized. Check your configuration above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework-Based Analysis Tool\n",
    "\n",
    "Create a tool that uses our XML Analysis Framework to prepare data for AI consumption. This tool itself doesn't use AI - it structures XML data for AI processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XML Analysis Framework Tool initialized (prepares data for AI consumption)\n"
     ]
    }
   ],
   "source": [
    "class XMLAnalysisFrameworkTool:\n",
    "    \"\"\"📊 Framework-Prepared: Uses XML Analysis Framework to structure data for AI consumption\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.framework = xaf\n",
    "        self.analysis_history = []\n",
    "    \n",
    "    def analyze_document_for_ai(self, file_path: str) -> dict:\n",
    "        \"\"\"Prepare comprehensive XML analysis for AI agent consumption\"\"\"\n",
    "        try:\n",
    "            # Use our framework to structure the data\n",
    "            result = self.framework.analyze(file_path)\n",
    "            enhanced = self.framework.analyze_enhanced(file_path) \n",
    "            schema = self.framework.analyze_schema(file_path)\n",
    "            chunks = self.framework.chunk(file_path, strategy=\"auto\")\n",
    "            \n",
    "            # Structure the data for AI consumption (this is NOT AI-generated)\n",
    "            structured_analysis = {\n",
    "                'document_summary': {\n",
    "                    'file_path': file_path,\n",
    "                    'document_type': result['document_type'].type_name,\n",
    "                    'confidence': result['document_type'].confidence,\n",
    "                    'handler_used': result['handler_used']\n",
    "                },\n",
    "                'technical_details': {\n",
    "                    'total_elements': schema.total_elements,\n",
    "                    'max_depth': schema.max_depth,\n",
    "                    'root_element': schema.root_element,\n",
    "                    'namespace_count': len(schema.namespaces)\n",
    "                },\n",
    "                'ai_ready_insights': {\n",
    "                    'identified_use_cases': enhanced.ai_use_cases,\n",
    "                    'key_findings': enhanced.key_findings,\n",
    "                    'quality_score': enhanced.quality_metrics,\n",
    "                    'extracted_data': enhanced.structured_data\n",
    "                },\n",
    "                'processing_results': {\n",
    "                    'chunk_count': len(chunks),\n",
    "                    'total_tokens': sum(c.token_estimate for c in chunks),\n",
    "                    'sample_chunks': [\n",
    "                        {\n",
    "                            'id': c.chunk_id,\n",
    "                            'path': c.element_path,\n",
    "                            'content_preview': c.content[:200] + \"...\" if len(c.content) > 200 else c.content,\n",
    "                            'token_count': c.token_estimate\n",
    "                        } for c in chunks[:3]  # First 3 chunks only\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            self.analysis_history.append(structured_analysis)\n",
    "            return structured_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e), 'file_path': file_path}\n",
    "    \n",
    "    def get_analysis_summary_text(self, analysis: dict) -> str:\n",
    "        \"\"\"Create human-readable summary of framework analysis (NOT AI-generated)\"\"\"\n",
    "        if 'error' in analysis:\n",
    "            return f\"Error analyzing {analysis['file_path']}: {analysis['error']}\"\n",
    "        \n",
    "        summary = f\"\"\"Document Analysis Summary (Framework-Generated):\n",
    "\n",
    "File: {analysis['document_summary']['file_path']}\n",
    "Type: {analysis['document_summary']['document_type']} \n",
    "Confidence: {analysis['document_summary']['confidence']:.1%}\n",
    "Handler: {analysis['document_summary']['handler_used']}\n",
    "\n",
    "Structure:\n",
    "- Elements: {analysis['technical_details']['total_elements']}\n",
    "- Max Depth: {analysis['technical_details']['max_depth']}\n",
    "- Root: {analysis['technical_details']['root_element']}\n",
    "- Namespaces: {analysis['technical_details']['namespace_count']}\n",
    "\n",
    "AI Processing Ready:\n",
    "- Generated {analysis['processing_results']['chunk_count']} chunks\n",
    "- Total tokens: {analysis['processing_results']['total_tokens']}\n",
    "- Identified use cases: {len(analysis['ai_ready_insights']['identified_use_cases'])}\n",
    "\"\"\"\n",
    "        return summary\n",
    "\n",
    "# Initialize the framework tool\n",
    "xml_tool = XMLAnalysisFrameworkTool()\n",
    "print(\"✅ XML Analysis Framework Tool initialized (prepares data for AI consumption)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Framework Analysis (Data Preparation)\n",
    "\n",
    "Run our framework analysis to prepare structured data for AI consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Framework-Prepared: Running XML analysis to structure data for AI...\n",
      "File size: 0.0 MB\n",
      "Using iterative parsing for large file: data/mapbox-example.kml\n",
      "✅ Framework analysis completed successfully!\n",
      "\n",
      "📊 Framework-Prepared Data Structure:\n",
      "{\n",
      "  \"file_path\": \"data/mapbox-example.kml\",\n",
      "  \"document_type\": \"KML Geographic Data\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"handler_used\": \"KMLHandler\"\n",
      "}\n",
      "\n",
      "Generated 23 chunks for AI processing\n"
     ]
    }
   ],
   "source": [
    "# Analyze our test KML file using the framework\n",
    "test_file = \"data/mapbox-example.kml\"\n",
    "\n",
    "# Check if file exists first\n",
    "import os\n",
    "if not os.path.exists(test_file):\n",
    "    print(f\"⚠️ Test file not found: {test_file}\")\n",
    "    print(\"Available files in data directory:\")\n",
    "    if os.path.exists(\"data\"):\n",
    "        for f in os.listdir(\"data\"):\n",
    "            if f.endswith('.xml') or f.endswith('.kml'):\n",
    "                print(f\"  - {f}\")\n",
    "        # Use the first available file\n",
    "        available_files = [f for f in os.listdir(\"data\") if f.endswith(('.xml', '.kml'))]\n",
    "        if available_files:\n",
    "            test_file = f\"data/{available_files[0]}\"\n",
    "            print(f\"Using: {test_file}\")\n",
    "    else:\n",
    "        print(\"Data directory not found. Creating a simple test...\")\n",
    "        # Create a simple XML for testing\n",
    "        test_file = \"simple_test.xml\"\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write('''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<root>\n",
    "    <item id=\"1\">\n",
    "        <name>Test Item 1</name>\n",
    "        <description>This is a test description</description>\n",
    "    </item>\n",
    "    <item id=\"2\">\n",
    "        <name>Test Item 2</name>\n",
    "        <description>Another test description</description>\n",
    "    </item>\n",
    "</root>''')\n",
    "        print(f\"Created simple test file: {test_file}\")\n",
    "\n",
    "print(\"📊 Framework-Prepared: Running XML analysis to structure data for AI...\")\n",
    "analysis = xml_tool.analyze_document_for_ai(test_file)\n",
    "\n",
    "# Display the structured analysis (prepared by framework, not AI)\n",
    "if 'error' not in analysis:\n",
    "    print(\"✅ Framework analysis completed successfully!\")\n",
    "    print(\"\\n📊 Framework-Prepared Data Structure:\")\n",
    "    print(json.dumps(analysis['document_summary'], indent=2))\n",
    "    print(f\"\\nGenerated {analysis['processing_results']['chunk_count']} chunks for AI processing\")\n",
    "else:\n",
    "    print(f\"❌ Framework analysis failed: {analysis['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Framework Summary (Non-AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Framework-Prepared Summary (NOT AI-generated):\n",
      "==================================================\n",
      "Document Analysis Summary (Framework-Generated):\n",
      "\n",
      "File: data/mapbox-example.kml\n",
      "Type: KML Geographic Data \n",
      "Confidence: 95.0%\n",
      "Handler: KMLHandler\n",
      "\n",
      "Structure:\n",
      "- Elements: 24\n",
      "- Max Depth: 6\n",
      "- Root: kml\n",
      "- Namespaces: 1\n",
      "\n",
      "AI Processing Ready:\n",
      "- Generated 23 chunks\n",
      "- Total tokens: 2289\n",
      "- Identified use cases: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a framework-based summary (NOT AI-generated)\n",
    "print(\"📊 Framework-Prepared Summary (NOT AI-generated):\")\n",
    "print(\"=\" * 50)\n",
    "summary = xml_tool.get_analysis_summary_text(analysis)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Real AI Agent Integration\n",
    "\n",
    "Now we create a real LangChain agent that can actually analyze our framework-prepared data using AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AI system created (optimized for 128k+ context models):\n",
      "  📊 Framework analysis tool (structures XML data)\n",
      "  🤖 Use-case specific AI tools (large context aware)\n",
      "  📄 Full document analysis capability\n",
      "  💾 Max content size: 100,000 characters\n",
      "  🔧 Response handler for various LangChain formats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rk/l45jnpxn3g1b9865j26zbbmr0000gn/T/ipykernel_1285/2093350089.py:200: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# Create AI tools that work WITH framework analysis, not replace it\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def xml_analysis_tool_func(file_path: str) -> str:\n",
    "    \"\"\"📊 Framework-Prepared: Analyze XML and structure for AI consumption\"\"\"\n",
    "    analysis = xml_tool.analyze_document_for_ai(file_path)\n",
    "    if 'error' in analysis:\n",
    "        return f\"Error analyzing {file_path}: {analysis['error']}\"\n",
    "    \n",
    "    # Return structured framework analysis for AI to work with\n",
    "    return xml_tool.get_analysis_summary_text(analysis)\n",
    "\n",
    "# Define the tool for LangChain that provides framework analysis\n",
    "xml_analysis_tool = Tool(\n",
    "    name=\"XML_Framework_Analysis\",\n",
    "    func=xml_analysis_tool_func,\n",
    "    description=\"Get structured XML document analysis including document type, metadata, chunks, and suggested AI use cases. Input: file path.\"\n",
    ")\n",
    "\n",
    "# Helper function to extract content from LangChain responses\n",
    "def extract_llm_content(response):\n",
    "    \"\"\"Extract text content from various LangChain response types\"\"\"\n",
    "    if hasattr(response, 'content'):\n",
    "        # AIMessage object\n",
    "        return response.content\n",
    "    elif hasattr(response, 'text'):\n",
    "        # Some models return .text\n",
    "        return response.text\n",
    "    elif isinstance(response, str):\n",
    "        # Already a string\n",
    "        return response\n",
    "    elif isinstance(response, dict) and 'content' in response:\n",
    "        # Dictionary response\n",
    "        return response['content']\n",
    "    else:\n",
    "        # Fallback\n",
    "        return str(response)\n",
    "\n",
    "# Create use-case-specific AI tools optimized for modern 128k+ context models\n",
    "class UseCase_SpecificAI:\n",
    "    \"\"\"🤖 AI-Generated responses for specific use cases - optimized for large context models\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.max_context_chars = 100000  # ~25k tokens, well within 128k limit\n",
    "        \n",
    "    def geospatial_analysis(self, chunks_content: str, specific_task: str) -> str:\n",
    "        \"\"\"🤖 AI for geospatial use cases - uses full document context\"\"\"\n",
    "        # With 128k context, we can analyze much more content at once\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in geospatial analysis. Based on the comprehensive XML content below, provide detailed insights for: {specific_task}\n",
    "\n",
    "XML Content (full document sections):\n",
    "{chunks_content[:self.max_context_chars]}\n",
    "\n",
    "Please provide a detailed JSON response with this structure:\n",
    "{{\n",
    "    \"task\": \"{specific_task}\",\n",
    "    \"insights\": [\"detailed insight 1\", \"detailed insight 2\", \"detailed insight 3\", \"...more as relevant\"],\n",
    "    \"patterns_identified\": [\"pattern 1\", \"pattern 2\", \"...\"],\n",
    "    \"recommendations\": [\"specific recommendation 1\", \"specific recommendation 2\", \"...\"],\n",
    "    \"extracted_features\": {{\n",
    "        \"locations\": [{{\n",
    "            \"name\": \"location name\",\n",
    "            \"coordinates\": {{\"lat\": 0.0, \"lon\": 0.0}},\n",
    "            \"type\": \"feature type\",\n",
    "            \"attributes\": {{}}\n",
    "        }}],\n",
    "        \"relationships\": [\"relationship 1\", \"relationship 2\"]\n",
    "    }},\n",
    "    \"summary\": \"comprehensive summary of findings\",\n",
    "    \"confidence\": 0.95\n",
    "}}\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return extract_llm_content(response)\n",
    "        except Exception as e:\n",
    "            return f'{{\"error\": \"{e}\"}}'\n",
    "    \n",
    "    def security_compliance_analysis(self, chunks_content: str, specific_task: str) -> str:\n",
    "        \"\"\"🤖 AI for security and compliance - comprehensive analysis\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "You are a security compliance expert. Based on the comprehensive XML content below, provide detailed analysis for: {specific_task}\n",
    "\n",
    "XML Content (full document):\n",
    "{chunks_content[:self.max_context_chars]}\n",
    "\n",
    "Please provide a comprehensive JSON response with this structure:\n",
    "{{\n",
    "    \"task\": \"{specific_task}\",\n",
    "    \"security_findings\": [\n",
    "        {{\"finding\": \"description\", \"severity\": \"low|medium|high|critical\", \"evidence\": \"specific XML elements\"}}\n",
    "    ],\n",
    "    \"compliance_status\": {{\n",
    "        \"overall\": \"compliant|non-compliant|partially-compliant\",\n",
    "        \"by_standard\": {{\"standard_name\": \"status\"}}\n",
    "    }},\n",
    "    \"vulnerabilities\": [\n",
    "        {{\"type\": \"vulnerability type\", \"description\": \"details\", \"mitigation\": \"recommended action\"}}\n",
    "    ],\n",
    "    \"risk_assessment\": {{\n",
    "        \"overall_risk\": \"low|medium|high\",\n",
    "        \"risk_factors\": [\"factor 1\", \"factor 2\"],\n",
    "        \"risk_score\": 0.0\n",
    "    }},\n",
    "    \"recommendations\": [\n",
    "        {{\"priority\": \"high|medium|low\", \"action\": \"specific action\", \"rationale\": \"why this matters\"}}\n",
    "    ],\n",
    "    \"confidence\": 0.95\n",
    "}}\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return extract_llm_content(response)\n",
    "        except Exception as e:\n",
    "            return f'{{\"error\": \"{e}\"}}'\n",
    "    \n",
    "    def dependency_analysis(self, chunks_content: str, specific_task: str) -> str:\n",
    "        \"\"\"🤖 AI for dependency analysis - full graph analysis\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "You are a software dependency expert. Based on the comprehensive XML content below, provide detailed analysis for: {specific_task}\n",
    "\n",
    "XML Content (full dependency tree):\n",
    "{chunks_content[:self.max_context_chars]}\n",
    "\n",
    "Please provide a comprehensive JSON response with this structure:\n",
    "{{\n",
    "    \"task\": \"{specific_task}\",\n",
    "    \"dependency_graph\": {{\n",
    "        \"total_dependencies\": 0,\n",
    "        \"direct_dependencies\": [\"dep1\", \"dep2\"],\n",
    "        \"transitive_dependencies\": [\"dep3\", \"dep4\"],\n",
    "        \"dependency_tree\": {{}}\n",
    "    }},\n",
    "    \"security_analysis\": {{\n",
    "        \"vulnerable_dependencies\": [\n",
    "            {{\"name\": \"dep\", \"version\": \"1.0\", \"vulnerabilities\": [\"CVE-...\"], \"severity\": \"high\"}}\n",
    "        ],\n",
    "        \"outdated_dependencies\": [\n",
    "            {{\"name\": \"dep\", \"current\": \"1.0\", \"latest\": \"2.0\", \"type\": \"major|minor|patch\"}}\n",
    "        ]\n",
    "    }},\n",
    "    \"compatibility_analysis\": {{\n",
    "        \"conflicts\": [\"conflict description\"],\n",
    "        \"version_constraints\": {{\"dep\": \"constraint\"}},\n",
    "        \"compatibility_score\": 0.95\n",
    "    }},\n",
    "    \"recommendations\": [\n",
    "        {{\"type\": \"upgrade|remove|replace\", \"dependency\": \"name\", \"action\": \"specific action\", \"impact\": \"expected impact\"}}\n",
    "    ],\n",
    "    \"confidence\": 0.95\n",
    "}}\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return extract_llm_content(response)\n",
    "        except Exception as e:\n",
    "            return f'{{\"error\": \"{e}\"}}'\n",
    "    \n",
    "    def comprehensive_document_analysis(self, full_analysis: dict, all_chunks: list, specific_focus: str = None) -> str:\n",
    "        \"\"\"🤖 Analyze entire document with all chunks - leverages large context window\"\"\"\n",
    "        # Combine all chunks for comprehensive analysis\n",
    "        full_content = \"\\n\\n=== CHUNK BOUNDARY ===\\n\\n\".join([\n",
    "            f\"Chunk {i+1} (Path: {chunk.element_path}):\\n{chunk.content}\" \n",
    "            for i, chunk in enumerate(all_chunks)\n",
    "        ])\n",
    "        \n",
    "        # Use full document metadata from framework\n",
    "        doc_type = full_analysis['document_summary']['document_type']\n",
    "        use_cases = full_analysis['ai_ready_insights']['identified_use_cases']\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are analyzing a complete {doc_type} document. The framework has identified these potential use cases:\n",
    "{', '.join(use_cases)}\n",
    "\n",
    "{\"Focus on: \" + specific_focus if specific_focus else \"Provide comprehensive analysis.\"}\n",
    "\n",
    "Full Document Content ({len(all_chunks)} chunks):\n",
    "{full_content[:self.max_context_chars]}\n",
    "\n",
    "Provide a comprehensive JSON analysis covering all relevant aspects for the identified use cases.\n",
    "Include specific examples from the document content to support your analysis.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return extract_llm_content(response)\n",
    "        except Exception as e:\n",
    "            return f'{{\"error\": \"{e}\", \"content_size\": {len(full_content)}}}'\n",
    "\n",
    "# Initialize use-case specific AI if LLM is available\n",
    "if 'llm' in locals():\n",
    "    usecase_ai = UseCase_SpecificAI(llm)\n",
    "    \n",
    "    # Create the main agent with framework analysis tool\n",
    "    from langchain.agents import initialize_agent, AgentType\n",
    "    \n",
    "    tools = [xml_analysis_tool]\n",
    "    \n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    \n",
    "    print(\"✅ AI system created (optimized for 128k+ context models):\")\n",
    "    print(\"  📊 Framework analysis tool (structures XML data)\")\n",
    "    print(\"  🤖 Use-case specific AI tools (large context aware)\")\n",
    "    print(\"  📄 Full document analysis capability\")\n",
    "    print(f\"  💾 Max content size: {usecase_ai.max_context_chars:,} characters\")\n",
    "    print(\"  🔧 Response handler for various LangChain formats\")\n",
    "else:\n",
    "    print(\"❌ LLM not initialized. Check your API key setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Framework-Guided AI Analysis\n",
    "\n",
    "The framework identifies the document type and suggests AI use cases. Now we use AI to work with that structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Framework Analysis Results:\n",
      "==================================================\n",
      "File size: 0.0 MB\n",
      "Using iterative parsing for large file: data/mapbox-example.kml\n",
      "Document Type: KML Geographic Data\n",
      "Suggested AI Use Cases: ['Geospatial pattern recognition', 'Location clustering and classification', 'Route optimization and analysis', 'Geographic feature extraction', 'Spatial relationship discovery', 'Area and distance calculations', 'Terrain and elevation analysis', 'Geographic anomaly detection', 'Location-based recommendations']\n",
      "Available chunks: 23\n",
      "\n",
      "📊 Creating chunks using auto strategy...\n",
      "✅ Created 23 chunks successfully\n",
      "📊 Total content: 61,332 characters (~2,289 tokens)\n",
      "📊 Framework prepared ALL 23 chunks for AI analysis\n",
      "\n",
      "🤖 AI Analysis of Framework-Prepared Data (Full Document):\n",
      "==================================================\n",
      "🤖 Running comprehensive document analysis...\n",
      "Comprehensive analysis result:\n",
      "To provide a comprehensive JSON analysis focusing on geospatial patterns and features from the KML document, we need to extract and analyze the relevant data points and structures. The document contains placemarks with point coordinates and a polygon, which can be used to identify geospatial patterns and features. Here's a structured JSON analysis:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"geospatial_analysis\": {\n",
      "    \"placemarks\": [\n",
      "      {\n",
      "        \"name\": \"Portland\",\n",
      "        \"coordinates\": {\n",
      "          \"longitude\": -122.681944,\n",
      "          \"latitude\": 45.52,\n",
      "          \"altitude\": 0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Rio de Janeiro\",\n",
      "        \"coordinates\": {\n",
      "          \"longitude\": -43.196389,\n",
      "          \"latitude\": -22.908333,\n",
      "          \"altitude\": 0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Istanbul\",\n",
      "        \"coordinates\": {\n",
      "          \"longitude\": 28.976018,\n",
      "          \"latitude\": 41.01224,\n",
      "          \"altitude\": 0\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Reykjavik\",\n",
      "        \"coordinates\": {\n",
      "          \"...\n",
      "\n",
      "🤖 Running full geospatial analysis (all chunks):\n",
      "```json\n",
      "{\n",
      "    \"task\": \"Comprehensive geospatial pattern recognition across entire document\",\n",
      "    \"insights\": [\n",
      "        \"The document contains multiple instances of geospatial data for four major cities: Portland, Rio de Janeiro, Istanbul, and Reykjavik.\",\n",
      "        \"A simple polygon is defined using the coordinates of these cities, suggesting a conceptual or analytical connection between them.\",\n",
      "        \"The document is highly repetitive, indicating redundancy in the data representation.\"\n",
      "    ],\n",
      "    \"patterns_identified\": [\n",
      "        \"Repetition of geospatial data for the same locations across multiple sections.\",\n",
      "        \"Use of a polygon to connect disparate global locations, possibly for comparative analysis.\"\n",
      "    ],\n",
      "    \"recommendations\": [\n",
      "        \"Consolidate repeated geospatial data to improve document efficiency and readability.\",\n",
      "        \"Investigate the purpose of the polygon connecting these cities to understand the analytical context.\"\n",
      "    ],\n",
      "    \"extracted_features\": {\n",
      "        \"locations\": [\n",
      "            {\n",
      "                \"name\": \"Portland\",\n",
      "                \"coordinates\": {\"lat\": 45.52, \"lon\": -122.681944},\n",
      "                \"type\": \"Point\",\n",
      "                \"attributes\": {}\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Rio de Janeiro\",\n",
      "                \"coordinates\": {\"lat\": -22.908333, \"lon\": -43.196389},\n",
      "                \"type\": \"Point\",\n",
      "                \"attributes\": {}\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Istanbul\",\n",
      "                \"coordinates\": {\"lat\": 41.01224, \"lon\": 28.976018},\n",
      "                \"type\": \"Point\",\n",
      "                \"attributes\": {}\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Reykjavik\",\n",
      "                \"coordinates\": {\"lat\": 64.133333, \"lon\": -21.933333},\n",
      "                \"type\": \"Point\",\n",
      "                \"attributes\": {}\n",
      "            }\n",
      "        ],\n",
      "        \"relationships\": [\n",
      "            \"The cities are connected by a polygon, indicating a potential analytical relationship.\"\n",
      "        ]\n",
      "    },\n",
      "    \"summary\": \"The document contains geospatial data for four cities, repeatedly listed, and connected by a polygon. This suggests a potential analytical framework or study involving these locations.\",\n",
      "    \"confidence\": 0.95\n",
      "}\n",
      "```...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Framework analysis (not AI - just data structuring)\n",
    "print(\"📊 Framework Analysis Results:\")\n",
    "print(\"=\" * 50)\n",
    "framework_analysis = xml_tool.analyze_document_for_ai(test_file)\n",
    "print(f\"Document Type: {framework_analysis['document_summary']['document_type']}\")\n",
    "print(f\"Suggested AI Use Cases: {framework_analysis['ai_ready_insights']['identified_use_cases']}\")\n",
    "print(f\"Available chunks: {framework_analysis['processing_results']['chunk_count']}\")\n",
    "\n",
    "# Step 2: Get framework-prepared content for AI to work with\n",
    "if 'error' not in framework_analysis:\n",
    "    # Create chunks using framework\n",
    "    print(\"\\n📊 Creating chunks using auto strategy...\")\n",
    "    all_chunks = xaf.chunk(test_file, strategy=\"auto\")\n",
    "    print(f\"✅ Created {len(all_chunks)} chunks successfully\")\n",
    "    \n",
    "    # With 128k context, we can use ALL chunks!\n",
    "    total_content_size = sum(len(chunk.content) for chunk in all_chunks)\n",
    "    total_tokens = sum(chunk.token_estimate for chunk in all_chunks)\n",
    "    print(f\"📊 Total content: {total_content_size:,} characters (~{total_tokens:,} tokens)\")\n",
    "    \n",
    "    # Combine all chunks for comprehensive analysis\n",
    "    chunks_content = \"\\n\\n=== CHUNK ===\\n\\n\".join([chunk.content for chunk in all_chunks])\n",
    "    \n",
    "    print(f\"📊 Framework prepared ALL {len(all_chunks)} chunks for AI analysis\")\n",
    "    \n",
    "    # Step 3: Use AI for comprehensive analysis with large context\n",
    "    if 'usecase_ai' in locals():\n",
    "        ai_use_cases = framework_analysis['ai_ready_insights']['identified_use_cases']\n",
    "        \n",
    "        print(\"\\n🤖 AI Analysis of Framework-Prepared Data (Full Document):\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Use comprehensive document analysis\n",
    "        print(\"🤖 Running comprehensive document analysis...\")\n",
    "        comprehensive_result = usecase_ai.comprehensive_document_analysis(\n",
    "            framework_analysis, \n",
    "            all_chunks,\n",
    "            specific_focus=\"geospatial patterns and features\"\n",
    "        )\n",
    "        print(\"Comprehensive analysis result:\")\n",
    "        print(comprehensive_result[:1000] + \"...\" if len(str(comprehensive_result)) > 1000 else comprehensive_result)\n",
    "        \n",
    "        # Example: Full geospatial analysis with all content\n",
    "        if any('geospatial' in uc.lower() or 'geographic' in uc.lower() for uc in ai_use_cases):\n",
    "            print(\"\\n🤖 Running full geospatial analysis (all chunks):\")\n",
    "            geo_analysis = usecase_ai.geospatial_analysis(chunks_content, \"Comprehensive geospatial pattern recognition across entire document\")\n",
    "            print(geo_analysis[:10000] + \"...\" if len(str(geo_analysis)) > 1000 else geo_analysis)\n",
    "            \n",
    "    else:\n",
    "        print(\"❌ Use-case specific AI not available\")\n",
    "else:\n",
    "    print(\"❌ Framework analysis failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Vector Database Preparation (Framework-Only)\n",
    "\n",
    "This section shows how to prepare the framework analysis for vector database ingestion. No AI is used here - just data structuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Framework-Prepared Vector Database Documents:\n",
      "Structured 23 documents for vector database ingestion\n",
      "\n",
      "Sample document structure (framework-prepared):\n",
      "{\n",
      "  \"id\": \"data/mapbox-example.kml_chunk_0_ac4e63fc\",\n",
      "  \"content\": \"<ns0:kml xmlns:ns0=\\\"http://www.opengis.net/kml/2.2\\\">\\n  <ns0:Document>\\n    <ns0:Placemark>\\n      <ns0:name>Portland</ns0:name>\\n      <ns0:Point>\\n        <ns0:coordinates>-122.681944,45.52,0</ns0:coordinates>\\n      </ns0:Point>\\n    </ns0:Placemark>\\n    <ns0:Placemark>\\n      <ns0:name>Rio de Janeiro</ns0:name>\\n      <ns0:Point>\\n        <ns0:coordinates>-43.196389,-22.908333,0</ns0:coordinates>\\n      </ns0:Point>\\n    </ns0:Placemark>\\n    <ns0:Placemark>\\n      <ns0:name>Istanbul</ns0:name>\\n      <ns0:Point>\\n        <ns0:coordinates>28.976018,41.01224,0</ns0:coordinates>\\n      </ns0:Point>\\n    </ns0:Placemark>\\n    <ns0:Placemark>\\n      <ns0:name>Reykjavik</ns0:name>\\n      <ns0:Point>\\n        <ns0:coordin...\n",
      "\n",
      "First chunk content preview:\n",
      "ID: data/mapbox-example.kml_chunk_0_ac4e63fc\n",
      "Content: <ns0:kml xmlns:ns0=\"http://www.opengis.net/kml/2.2\">\n",
      "  <ns0:Document>\n",
      "    <ns0:Placemark>\n",
      "      <ns0:name>Portland</ns0:name>\n",
      "      <ns0:Point>\n",
      "        <ns0:coordinates>-122.681944,45.52,0</ns0:coordi...\n",
      "Token estimate: 271\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_vector_db(analysis, chunks):\n",
    "    \"\"\"📊 Framework-Prepared: Structure data for vector database ingestion (NOT AI-generated)\"\"\"\n",
    "    \n",
    "    vector_docs = []\n",
    "    \n",
    "    # Document-level metadata (structured by framework)\n",
    "    doc_metadata = {\n",
    "        'document_type': analysis['document_summary']['document_type'],\n",
    "        'confidence': analysis['document_summary']['confidence'],\n",
    "        'handler_used': analysis['document_summary']['handler_used'],\n",
    "        'total_elements': analysis['technical_details']['total_elements'],\n",
    "        'max_depth': analysis['technical_details']['max_depth'],\n",
    "        'ai_use_cases': analysis['ai_ready_insights']['identified_use_cases'],\n",
    "        'processing_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Create vector documents for each chunk (framework-structured)\n",
    "    for chunk in chunks:\n",
    "        vector_doc = {\n",
    "            'id': f\"{analysis['document_summary']['file_path']}_{chunk.chunk_id}\",\n",
    "            'content': chunk.content,\n",
    "            'metadata': {\n",
    "                **doc_metadata,\n",
    "                'chunk_id': chunk.chunk_id,\n",
    "                'element_path': chunk.element_path,\n",
    "                'start_line': chunk.start_line,\n",
    "                'end_line': chunk.end_line,\n",
    "                'elements_included': chunk.elements_included,\n",
    "                'token_estimate': chunk.token_estimate,\n",
    "                'chunk_metadata': chunk.metadata\n",
    "            }\n",
    "        }\n",
    "        vector_docs.append(vector_doc)\n",
    "    \n",
    "    return vector_docs\n",
    "\n",
    "# Use the chunks we created with auto strategy (should work now)\n",
    "if 'all_chunks' in locals() and 'framework_analysis' in locals() and len(all_chunks) > 0:\n",
    "    vector_docs = prepare_for_vector_db(framework_analysis, all_chunks)\n",
    "    \n",
    "    print(\"📊 Framework-Prepared Vector Database Documents:\")\n",
    "    print(f\"Structured {len(vector_docs)} documents for vector database ingestion\")\n",
    "    \n",
    "    print(f\"\\nSample document structure (framework-prepared):\")\n",
    "    print(json.dumps(vector_docs[0], indent=2, default=str)[:800] + \"...\")\n",
    "    \n",
    "    print(f\"\\nFirst chunk content preview:\")\n",
    "    print(f\"ID: {vector_docs[0]['id']}\")\n",
    "    print(f\"Content: {vector_docs[0]['content'][:200]}...\")\n",
    "    print(f\"Token estimate: {vector_docs[0]['metadata']['token_estimate']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Chunks or framework analysis not available. Run previous cells first.\")\n",
    "    if 'all_chunks' in locals():\n",
    "        print(f\"Available chunks: {len(all_chunks)}\")\n",
    "    else:\n",
    "        print(\"all_chunks variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Structured AI Q&A System\n",
    "\n",
    "Ask specific questions about the framework-prepared data using structured prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Structured AI Q&A (working with framework data):\n",
      "============================================================\n",
      "\n",
      "**Question 1:** What geographic coordinates or locations can you extract from this data?\n",
      "**Expected Format:** json\n",
      "**🤖 AI Response (json):**\n",
      "Based on the provided KML data, I can extract the geographic coordinates and locations as follows:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"locations\": [\n",
      "    {\n",
      "      \"name\": \"Portland\",\n",
      "      \"coordinates\": {\n",
      "        \"longitude\": -122.681944,\n",
      "        \"latitude\": 45.52,\n",
      "        \"altitude\": 0\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Rio de Janeiro\",\n",
      "      \"coordinates\": {\n",
      "        \"longitude\": -43.196389,\n",
      "        \"latitude\": -22.908333,\n",
      "        \"altitude\": 0\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Istanbul\",\n",
      "      \"coordinates\": {\n",
      "        \"longitude\": 28.976018,\n",
      "        \"latitude\": 41.01224,\n",
      "        \"altitude\": 0\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Reykjavik\",\n",
      "      \"coordinates\": {\n",
      "        \"longitude\": -21.933333,\n",
      "        \"latitude\": 64.133333,\n",
      "        \"altitude\": 0\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Simple Polygon\",\n",
      "      \"polygon\": {\n",
      "        \"outerBoundaryCoordinates\": [\n",
      "          {\n",
      "            \"longitude\": -122.681944,\n",
      "            \"latitude\": 45.52,\n",
      "            \"altitude\": 0\n",
      "          },\n",
      "          {\n",
      "            \"longitude\": -43.196389,\n",
      "            \"latitude\": -22.908333,\n",
      "            \"altitude\": 0\n",
      "          },\n",
      "          {\n",
      "            \"longitude\": 28.976018,\n",
      "            \"latitude\": 41.01224,\n",
      "            \"altitude\": 0\n",
      "          },\n",
      "          {\n",
      "            \"longitude\": -21.933333,\n",
      "            \"latitude\": 64.133333,\n",
      "            \"altitude\": 0\n",
      "          },\n",
      "          {\n",
      "            \"longitude\": -122.681944,\n",
      "            \"latitude\": 45.52,\n",
      "            \"altitude\": 0\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "This JSON response includes the names and coordinates of the locations extracted from the KML data, as well as the coordinates that define the \"Simple Polygon\".\n",
      "----------------------------------------\n",
      "\n",
      "**Question 2:** What would be the best way to index this data for spatial search?\n",
      "**Expected Format:** structured text\n",
      "**🤖 AI Response (structured text):**\n",
      "To effectively index the KML Geographic Data document for spatial search, you should consider using a spatial database or a search engine with geospatial capabilities. Here’s a structured approach to achieve this:\n",
      "\n",
      "1. **Choose a Spatial Database or Search Engine:**\n",
      "   - **PostGIS**: An extension for PostgreSQL that adds support for geographic objects, allowing location queries to be run in SQL.\n",
      "   - **Elasticsearch with Geo Queries**: Elasticsearch supports geospatial data types and queries, which can be used to index and search geographic data.\n",
      "   - **MongoDB with Geospatial Indexing**: MongoDB provides geospatial indexing to store and query geospatial data.\n",
      "\n",
      "2. **Data Extraction and Transformation:**\n",
      "   - Parse the KML document to extract relevant geographic data, such as placemarks, points, and polygons.\n",
      "   - Convert the coordinates from the KML format into a format suitable for the chosen database or search engine. Typically, this involves converting the coordinates into a GeoJSON format or a similar structure.\n",
      "\n",
      "3. **Indexing Strategy:**\n",
      "   - **Points**: For each `<Placemark>` with a `<Point>`, create a document with fields for the name and the coordinates. Ensure the coordinates are stored in a geospatial data type.\n",
      "   - **Polygons**: For `<Placemark>` elements with `<Polygon>`, store the polygon coordinates as a geospatial polygon type. This allows for complex spatial queries like containment and intersection.\n",
      "\n",
      "4. **Index Creation:**\n",
      "   - Create a geospatial index on the coordinates field. This index will allow efficient querying of spatial data.\n",
      "   - For databases like PostGIS, use the `CREATE INDEX` statement with the `GIST` or `SP-GiST` index type.\n",
      "   - For Elasticsearch, define a mapping with a `geo_point` or `geo_shape` field type, depending on whether you are indexing points or polygons.\n",
      "\n",
      "5. **Querying:**\n",
      "   - Use spatial queries to perform operations such as finding all points within a certain radius, determining if a point is within a polygon, or finding the nearest neighbors.\n",
      "   - Examples of queries include:\n",
      "     - **Bounding Box Query**: Find all points within a specified rectangular area.\n",
      "     - **Radius Query**: Find all points within a certain distance from a given point.\n",
      "     - **Polygon Query**: Determine if a point lies within a specified polygon.\n",
      "\n",
      "6. **Optimization and Maintenance:**\n",
      "   - Regularly update the index to reflect any changes in the underlying data.\n",
      "   - Optimize the index periodically to ensure efficient query performance.\n",
      "\n",
      "By following this structured approach, you can effectively index and query the KML data for spatial search, enabling powerful geospatial analysis and retrieval capabilities.\n",
      "----------------------------------------\n",
      "\n",
      "**Question 3:** How would you classify the geographic features found in this document?\n",
      "**Expected Format:** json\n",
      "**🤖 AI Response (json):**\n",
      "```json\n",
      "{\n",
      "  \"geographic_features\": [\n",
      "    {\n",
      "      \"type\": \"Point\",\n",
      "      \"name\": \"Portland\",\n",
      "      \"coordinates\": [-122.681944, 45.52, 0]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"Point\",\n",
      "      \"name\": \"Rio de Janeiro\",\n",
      "      \"coordinates\": [-43.196389, -22.908333, 0]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"Point\",\n",
      "      \"name\": \"Istanbul\",\n",
      "      \"coordinates\": [28.976018, 41.01224, 0]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"Point\",\n",
      "      \"name\": \"Reykjavik\",\n",
      "      \"coordinates\": [-21.933333, 64.133333, 0]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"Polygon\",\n",
      "      \"name\": \"Simple Polygon\",\n",
      "      \"coordinates\": [\n",
      "        [-122.681944, 45.52, 0],\n",
      "        [-43.196389, -22.908333, 0],\n",
      "        [28.976018, 41.01224, 0],\n",
      "        [-21.933333, 64.133333, 0],\n",
      "        [-122.681944, 45.52, 0]\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create structured Q&A system that works with framework data\n",
    "if 'llm' in locals() and 'framework_analysis' in locals():\n",
    "    \n",
    "    def ask_structured_question(question: str, expected_format: str = \"json\") -> str:\n",
    "        \"\"\"🤖 AI-Generated: Ask specific questions with structured responses\"\"\"\n",
    "        \n",
    "        # Use framework-prepared metadata and chunks\n",
    "        doc_type = framework_analysis['document_summary']['document_type']\n",
    "        use_cases = framework_analysis['ai_ready_insights']['identified_use_cases']\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Based on the framework analysis below, answer the specific question with a structured response.\n",
    "\n",
    "FRAMEWORK ANALYSIS:\n",
    "Document Type: {doc_type}\n",
    "Suggested AI Use Cases: {use_cases}\n",
    "Content Chunks: {len(doc_chunks)} chunks analyzed\n",
    "\n",
    "CONTENT:\n",
    "{chunks_content[:2000]}...\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Please provide a {expected_format.upper()} response that directly answers the question based on the framework-analyzed content above.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            return extract_llm_content(response)\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    # Test structured questions based on framework insights\n",
    "    questions = [\n",
    "        {\n",
    "            \"question\": \"What geographic coordinates or locations can you extract from this data?\",\n",
    "            \"format\": \"json\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What would be the best way to index this data for spatial search?\",\n",
    "            \"format\": \"structured text\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How would you classify the geographic features found in this document?\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🤖 Structured AI Q&A (working with framework data):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"\\n**Question {i}:** {q['question']}\")\n",
    "        print(f\"**Expected Format:** {q['format']}\")\n",
    "        \n",
    "        # Create structured prompt for this question\n",
    "        structured_prompt = f\"\"\"\n",
    "You are analyzing XML data that has been processed by our framework. The framework identified this as a {framework_analysis['document_summary']['document_type']} document.\n",
    "\n",
    "Content to analyze:\n",
    "{chunks_content[:1500]}...\n",
    "\n",
    "Question: {q['question']}\n",
    "\n",
    "Please provide a {q['format']} response.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            ai_response = llm.invoke(structured_prompt)\n",
    "            ai_content = extract_llm_content(ai_response)  # Extract content from AIMessage\n",
    "            print(f\"**🤖 AI Response ({q['format']}):**\")\n",
    "            print(ai_content)\n",
    "        except Exception as e:\n",
    "            print(f\"**Error:** {e}\")\n",
    "            \n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "else:\n",
    "    print(\"❌ LLM or framework analysis not available for Q&A.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Advanced Use Cases with Large Context Models\n",
    "\n",
    "Leverage the 128k+ context window for sophisticated analysis patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Advanced Large Context Analysis Patterns:\n",
      "============================================================\n",
      "\n",
      "1️⃣ Cross-Reference Analysis Across All Chunks:\n",
      "\n",
      "2️⃣ Multi-Stage Analysis Pipeline:\n",
      "  - Stage 1: Entity extraction across all chunks\n",
      "  - Stage 2: Relationship mapping\n",
      "  - Stage 3: Insight generation\n",
      "\n",
      "3️⃣ Comparative Analysis Pattern:\n",
      "  Compare different sections of the document to find:\n",
      "  - Consistency in data formats\n",
      "  - Evolution of patterns from start to end\n",
      "  - Outliers and anomalies\n",
      "\n",
      "4️⃣ Multi-Level Summarization:\n",
      "  - Executive: 1-paragraph executive summary\n",
      "  - Technical: Technical summary with key data points\n",
      "  - Detailed: Detailed summary with examples from content\n",
      "\n",
      "5️⃣ Deep Dive Analysis for Each Use Case:\n",
      "  1. Geospatial pattern recognition\n",
      "  2. Location clustering and classification\n",
      "  3. Route optimization and analysis\n",
      "  4. Geographic feature extraction\n",
      "  5. Spatial relationship discovery\n",
      "\n",
      "🤖 Example: Running Cross-Reference Analysis...\n",
      "Result preview: The document is a KML (Keyhole Markup Language) file, which is used to represent geographic data. The document is structured into multiple chunks, each containing geographic information about specific locations and a polygon that connects these locations. Here's an analysis of the document's structure, cross-references, relationships, and patterns:\n",
      "\n",
      "### Structure and Content:\n",
      "1. **Placemark Elements**: The document contains multiple `<Placemark>` elements, each representing a geographic location with a name and coordinates. The locations mentioned are:\n",
      "   - Portland\n",
      "   - Rio de Janeiro\n",
      "   - Istanbul\n",
      "   - Reykjavik\n",
      "\n",
      "2. **Coordinates**: Each `<Placemark>` has a `<Point>` element containing `<coordinates>`, which specifies the longitude, latitude, and altitude (altitude is 0 for all points in this document).\n",
      "\n",
      "3. **Polygon Element**: There is a `<Placemark>` named \"Simple Polygon\" that contains a `<Polygon>` element. This polygon is defined by an `<outerBoundaryIs>` element, which includes a `<LinearRing>` with coordinates that connect the four locations mentioned above, forming a closed loop.\n",
      "\n",
      "### Cross-References and Relationships:\n",
      "- **Repeated Data**: The document contains repeated data across multiple chunks. Each location and its coordinates are repeated several times, indicating redundancy.\n",
      "- **Polygon and Points**: The \"Simple Polygon\" placemark references the same coordinates as the individual points for Portland, Rio de Janeiro, Istanbul, and Reykjavik, indicating that the polygon is formed by connecting these points.\n",
      "- **Namespace**: The document uses the namespace `http://www.opengis.net/kml/2.2`, which is standard for KML files.\n",
      "\n",
      "### Patterns:\n",
      "- **Repetition**: The document repeats the same set of placemarks and polygon definitions multiple times, suggesting either a need for emphasis or a structural redundancy.\n",
      "- **Geographic Connection**: The polygon connects the four cities, suggesting a geographic relationship or area of interest that encompasses these locations.\n",
      "- **Consistent Structure**: Each placemark follows a consistent structure with a name, point, and coordinates, which is typical for KML files to ensure compatibility with geographic information systems (GIS).\n",
      "\n",
      "### Potential Use Cases:\n",
      "- **Geographic Visualization**: The document can be used in GIS applications to visualize the locations and the polygon connecting them.\n",
      "- **Spatial Analysis**: The polygon could represent a region of interest for spatial analysis, such as climate studies, travel routes, or cultural connections.\n",
      "\n",
      "Overall, the document is a well-structured KML file that provides geographic data for visualization and analysis, albeit with some redundancy in its content....\n"
     ]
    }
   ],
   "source": [
    "# Advanced patterns that leverage large context windows\n",
    "if 'usecase_ai' in locals() and 'all_chunks' in locals():\n",
    "    \n",
    "    print(\"🚀 Advanced Large Context Analysis Patterns:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Cross-Reference Analysis\n",
    "    print(\"\\n1️⃣ Cross-Reference Analysis Across All Chunks:\")\n",
    "    cross_ref_prompt = f\"\"\"\n",
    "Analyze the entire document and identify cross-references, relationships, and patterns across all chunks:\n",
    "\n",
    "Document Type: {framework_analysis['document_summary']['document_type']}\n",
    "Total Chunks: {len(all_chunks)}\n",
    "\n",
    "Full Content:\n",
    "{chunks_content[:usecase_ai.max_context_chars]}\n",
    "\n",
    "Identify:\n",
    "1. Repeated patterns across different sections\n",
    "2. Related elements that reference each other\n",
    "3. Hierarchical relationships in the data\n",
    "4. Anomalies or inconsistencies across chunks\n",
    "\n",
    "Provide JSON response with findings.\n",
    "\"\"\"\n",
    "    \n",
    "    # 2. Multi-Stage Analysis Pipeline\n",
    "    print(\"\\n2️⃣ Multi-Stage Analysis Pipeline:\")\n",
    "    \n",
    "    # Stage 1: Extract all entities\n",
    "    entity_extraction_prompt = f\"\"\"\n",
    "Extract ALL entities from this {framework_analysis['document_summary']['document_type']} document:\n",
    "\n",
    "{chunks_content[:50000]}\n",
    "\n",
    "Return JSON with categorized entities.\n",
    "\"\"\"\n",
    "    \n",
    "    # Stage 2: Analyze relationships between entities\n",
    "    relationship_prompt = \"\"\"\n",
    "Based on the entities found, analyze their relationships and create a knowledge graph structure.\n",
    "\"\"\"\n",
    "    \n",
    "    # Stage 3: Generate insights\n",
    "    insight_prompt = \"\"\"\n",
    "Based on the entity relationships, generate actionable insights and recommendations.\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"  - Stage 1: Entity extraction across all chunks\")\n",
    "    print(\"  - Stage 2: Relationship mapping\")\n",
    "    print(\"  - Stage 3: Insight generation\")\n",
    "    \n",
    "    # 3. Comparative Analysis\n",
    "    print(\"\\n3️⃣ Comparative Analysis Pattern:\")\n",
    "    print(\"  Compare different sections of the document to find:\")\n",
    "    print(\"  - Consistency in data formats\")\n",
    "    print(\"  - Evolution of patterns from start to end\")\n",
    "    print(\"  - Outliers and anomalies\")\n",
    "    \n",
    "    # 4. Document Summarization at Multiple Levels\n",
    "    print(\"\\n4️⃣ Multi-Level Summarization:\")\n",
    "    summarization_levels = {\n",
    "        \"executive\": \"1-paragraph executive summary\",\n",
    "        \"technical\": \"Technical summary with key data points\",\n",
    "        \"detailed\": \"Detailed summary with examples from content\"\n",
    "    }\n",
    "    \n",
    "    for level, description in summarization_levels.items():\n",
    "        print(f\"  - {level.capitalize()}: {description}\")\n",
    "    \n",
    "    # 5. Use-Case-Specific Deep Dives\n",
    "    print(\"\\n5️⃣ Deep Dive Analysis for Each Use Case:\")\n",
    "    for i, use_case in enumerate(framework_analysis['ai_ready_insights']['identified_use_cases'][:5], 1):\n",
    "        print(f\"  {i}. {use_case}\")\n",
    "        # Each use case could have its own comprehensive analysis\n",
    "    \n",
    "    # Example: Run one of these analyses\n",
    "    try:\n",
    "        print(\"\\n🤖 Example: Running Cross-Reference Analysis...\")\n",
    "        # Truncate if needed, but we have lots of room with 128k context\n",
    "        cross_ref_result = llm.invoke(cross_ref_prompt[:50000])\n",
    "        cross_ref_content = extract_llm_content(cross_ref_result)  # Handle AIMessage\n",
    "        print(\"Result preview:\", str(cross_ref_content)[:5000] + \"...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in cross-reference analysis: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Required components not available for advanced analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates **proper division of labor** between framework and AI:\n",
    "\n",
    "### 📊 XML Analysis Framework Does:\n",
    "- **Document Type Detection**: Identifies KML, Maven POM, SCAP, etc.\n",
    "- **Data Structuring**: Extracts metadata, creates chunks, prepares schemas\n",
    "- **Use Case Identification**: Suggests specific AI applications per document type\n",
    "- **Content Preparation**: Optimizes data for AI consumption (chunking, token estimation)\n",
    "\n",
    "### 🤖 AI Does (working with framework data):\n",
    "- **Pattern Recognition**: Analyzes geospatial patterns in structured geographic data\n",
    "- **Feature Extraction**: Identifies specific geographic/business features from chunks\n",
    "- **Classification**: Categorizes content using framework-prepared metadata\n",
    "- **Insight Generation**: Provides domain-specific analysis based on use cases\n",
    "- **Structured Responses**: Returns JSON for downstream processing\n",
    "\n",
    "### 🔑 Key Benefits:\n",
    "1. **Efficiency**: Framework handles heavy XML parsing, AI focuses on insights\n",
    "2. **Scalability**: Smaller AI models can work with pre-structured data\n",
    "3. **Consistency**: Framework ensures reliable data preparation\n",
    "4. **Flexibility**: Use-case-specific AI workflows based on document type\n",
    "5. **Cost-Effective**: Reduces API calls by using framework for data prep\n",
    "\n",
    "### 🚀 Production Pattern:\n",
    "```python\n",
    "# 1. Framework analyzes and structures (fast, deterministic)\n",
    "analysis = xaf.analyze_enhanced(\"document.xml\")\n",
    "chunks = xaf.chunk(\"document.xml\", strategy=\"auto\")\n",
    "\n",
    "# 2. AI works with structured data (focused, efficient)\n",
    "for use_case in analysis.ai_use_cases:\n",
    "    ai_result = ai_workflow(use_case, chunks)\n",
    "    \n",
    "# 3. Combine framework metadata + AI insights\n",
    "final_result = {\n",
    "    'document_metadata': analysis,  # Framework-prepared\n",
    "    'ai_insights': ai_result        # AI-generated\n",
    "}\n",
    "```\n",
    "\n",
    "This approach allows smaller, faster models to work effectively since the framework handles the complex XML parsing and data structuring! 🎯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
