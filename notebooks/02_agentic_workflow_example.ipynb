{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Workflow with XML Analysis Framework\n",
    "\n",
    "This notebook demonstrates how to integrate the XML Analysis Framework into an AI agent workflow using LangChain. The example shows how an AI agent could:\n",
    "\n",
    "1. Analyze XML documents to understand their structure and content\n",
    "2. Make intelligent decisions about how to process the data\n",
    "3. Generate summaries and insights\n",
    "4. Create actionable recommendations\n",
    "\n",
    "**Note:** This is a conceptual example. You'll need to add your own API keys and customize for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install xml-analysis-framework\n",
    "!pip install langchain\n",
    "!pip install langchain-openai\n",
    "!pip install langchain-community\n",
    "\n",
    "import xml_analysis_framework as xaf\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"XML Analysis Framework version: {xaf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Agent Setup (Conceptual)\n",
    "\n",
    "Here's how you would set up LangChain agents to work with the XML analysis results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how you would set up LangChain (requires API keys)\n",
    "# Uncomment and configure when you have your API keys\n",
    "\n",
    "# from langchain.agents import initialize_agent, Tool\n",
    "# from langchain.llms import OpenAI\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.schema import SystemMessage\n",
    "\n",
    "# # Initialize LLM (you'll need to set OPENAI_API_KEY)\n",
    "# llm = OpenAI(temperature=0.1)\n",
    "\n",
    "# # Initialize memory\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "print(\"LangChain setup example shown above (commented out - requires API keys)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Analysis Agent Tool\n",
    "\n",
    "Create a custom tool that wraps the XML Analysis Framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XMLAnalysisAgent:\n",
    "    \"\"\"Agent that uses XML Analysis Framework to understand and process XML documents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.framework = xaf\n",
    "        self.analysis_history = []\n",
    "    \n",
    "    def analyze_document(self, file_path: str) -> dict:\n",
    "        \"\"\"Comprehensive XML document analysis\"\"\"\n",
    "        try:\n",
    "            # Step 1: Basic analysis\n",
    "            result = self.framework.analyze(file_path)\n",
    "            \n",
    "            # Step 2: Enhanced analysis\n",
    "            enhanced = self.framework.analyze_enhanced(file_path)\n",
    "            \n",
    "            # Step 3: Schema analysis\n",
    "            schema = self.framework.analyze_schema(file_path)\n",
    "            \n",
    "            # Step 4: Smart chunking\n",
    "            chunks = self.framework.chunk(file_path, strategy=\"auto\")\n",
    "            \n",
    "            # Compile comprehensive analysis\n",
    "            analysis = {\n",
    "                'document_info': {\n",
    "                    'file_path': file_path,\n",
    "                    'document_type': result['document_type'].type_name,\n",
    "                    'confidence': result['document_type'].confidence,\n",
    "                    'handler_used': result['handler_used']\n",
    "                },\n",
    "                'schema_info': {\n",
    "                    'total_elements': schema.total_elements,\n",
    "                    'max_depth': schema.max_depth,\n",
    "                    'root_element': schema.root_element,\n",
    "                    'namespaces': schema.namespaces\n",
    "                },\n",
    "                'ai_insights': {\n",
    "                    'use_cases': enhanced.ai_use_cases,\n",
    "                    'key_findings': enhanced.key_findings,\n",
    "                    'quality_metrics': enhanced.quality_metrics,\n",
    "                    'structured_data': enhanced.structured_data\n",
    "                },\n",
    "                'chunking_info': {\n",
    "                    'total_chunks': len(chunks),\n",
    "                    'avg_chunk_size': sum(len(c.content) for c in chunks) / len(chunks) if chunks else 0,\n",
    "                    'total_tokens': sum(c.token_estimate for c in chunks)\n",
    "                },\n",
    "                'chunks': chunks[:5]  # First 5 chunks for preview\n",
    "            }\n",
    "            \n",
    "            self.analysis_history.append(analysis)\n",
    "            return analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e), 'file_path': file_path}\n",
    "    \n",
    "    def generate_summary(self, analysis: dict) -> str:\n",
    "        \"\"\"Generate a human-readable summary of the analysis\"\"\"\n",
    "        if 'error' in analysis:\n",
    "            return f\"âŒ Error analyzing {analysis['file_path']}: {analysis['error']}\"\n",
    "        \n",
    "        doc_info = analysis['document_info']\n",
    "        schema_info = analysis['schema_info']\n",
    "        ai_insights = analysis['ai_insights']\n",
    "        chunking_info = analysis['chunking_info']\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "ðŸ“„ **Document Analysis Summary**\n",
    "\n",
    "**Document Type:** {doc_info['document_type']} (Confidence: {doc_info['confidence']:.1%})\n",
    "**Handler:** {doc_info['handler_used']}\n",
    "\n",
    "**Structure:**\n",
    "- Elements: {schema_info['total_elements']}\n",
    "- Max Depth: {schema_info['max_depth']}\n",
    "- Root: {schema_info['root_element']}\n",
    "- Namespaces: {len(schema_info['namespaces'])}\n",
    "\n",
    "**AI/ML Potential:**\n",
    "- Use Cases: {', '.join(ai_insights['use_cases'][:3])}{'...' if len(ai_insights['use_cases']) > 3 else ''}\n",
    "- Key Findings: {len(ai_insights['key_findings'])} items\n",
    "\n",
    "**Processing Results:**\n",
    "- Generated {chunking_info['total_chunks']} chunks\n",
    "- Average chunk size: {chunking_info['avg_chunk_size']:.0f} characters\n",
    "- Total tokens: ~{chunking_info['total_tokens']}\n",
    "\n",
    "**Recommendations:**\n",
    "This document is well-suited for: {', '.join(ai_insights['use_cases'][:2])}\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def suggest_processing_strategy(self, analysis: dict) -> dict:\n",
    "        \"\"\"Suggest how to best process this document for AI/ML applications\"\"\"\n",
    "        if 'error' in analysis:\n",
    "            return {'strategy': 'error', 'reason': analysis['error']}\n",
    "        \n",
    "        doc_type = analysis['document_info']['document_type'].lower()\n",
    "        use_cases = analysis['ai_insights']['use_cases']\n",
    "        chunks_count = analysis['chunking_info']['total_chunks']\n",
    "        \n",
    "        # Decision logic based on document type and characteristics\n",
    "        if 'security' in doc_type or 'scap' in doc_type:\n",
    "            return {\n",
    "                'strategy': 'security_analysis',\n",
    "                'recommended_tools': ['Vector DB for vulnerability search', 'Graph DB for dependency mapping'],\n",
    "                'chunking': 'hierarchical',\n",
    "                'embedding_focus': 'security findings and recommendations',\n",
    "                'ai_applications': ['Risk assessment', 'Compliance monitoring', 'Automated remediation']\n",
    "            }\n",
    "        elif 'kml' in doc_type or 'geographic' in doc_type:\n",
    "            return {\n",
    "                'strategy': 'geospatial_analysis',\n",
    "                'recommended_tools': ['Spatial databases', 'Vector embeddings for location queries'],\n",
    "                'chunking': 'content_aware',\n",
    "                'embedding_focus': 'geographic features and metadata',\n",
    "                'ai_applications': ['Location intelligence', 'Route optimization', 'Spatial search']\n",
    "            }\n",
    "        elif 'maven' in doc_type or 'build' in doc_type:\n",
    "            return {\n",
    "                'strategy': 'dependency_analysis',\n",
    "                'recommended_tools': ['Graph DB for dependency networks', 'Vector DB for similarity'],\n",
    "                'chunking': 'hierarchical',\n",
    "                'embedding_focus': 'dependencies and configurations',\n",
    "                'ai_applications': ['Dependency management', 'Upgrade planning', 'Security scanning']\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'strategy': 'general_content',\n",
    "                'recommended_tools': ['Vector DB for semantic search', 'Traditional text processing'],\n",
    "                'chunking': 'auto',\n",
    "                'embedding_focus': 'main content and structure',\n",
    "                'ai_applications': ['Content search', 'Document Q&A', 'Information extraction']\n",
    "            }\n",
    "\n",
    "# Initialize the agent\n",
    "xml_agent = XMLAnalysisAgent()\n",
    "print(\"âœ… XML Analysis Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Test Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze our test KML file\n",
    "test_file = \"data/mapbox-example.kml\"\n",
    "\n",
    "print(\"ðŸ” Starting comprehensive analysis...\")\n",
    "analysis = xml_agent.analyze_document(test_file)\n",
    "\n",
    "# Display the analysis\n",
    "if 'error' not in analysis:\n",
    "    print(\"âœ… Analysis completed successfully!\")\n",
    "    print(json.dumps(analysis['document_info'], indent=2))\n",
    "else:\n",
    "    print(f\"âŒ Analysis failed: {analysis['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate AI Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a human-readable summary\n",
    "summary = xml_agent.generate_summary(analysis)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Processing Strategy Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intelligent processing recommendations\n",
    "strategy = xml_agent.suggest_processing_strategy(analysis)\n",
    "print(\"ðŸŽ¯ **Recommended Processing Strategy:**\")\n",
    "print(json.dumps(strategy, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated LangChain Agent Workflow\n",
    "\n",
    "Here's how you might integrate this with actual LangChain agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedLangChainAgent:\n",
    "    \"\"\"Simulates how a LangChain agent would work with XML analysis results\"\"\"\n",
    "    \n",
    "    def __init__(self, xml_agent):\n",
    "        self.xml_agent = xml_agent\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def process_user_query(self, query: str, file_path: str = None) -> str:\n",
    "        \"\"\"Simulate how an agent would respond to user queries about XML documents\"\"\"\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # If a file is provided, analyze it first\n",
    "        if file_path:\n",
    "            analysis = self.xml_agent.analyze_document(file_path)\n",
    "            \n",
    "        # Simulate different types of responses based on query intent\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if \"analyze\" in query_lower or \"what is\" in query_lower:\n",
    "            if file_path:\n",
    "                response = f\"I've analyzed the XML document. {self.xml_agent.generate_summary(analysis)}\"\n",
    "            else:\n",
    "                response = \"Please provide an XML file to analyze.\"\n",
    "                \n",
    "        elif \"recommend\" in query_lower or \"strategy\" in query_lower:\n",
    "            if file_path:\n",
    "                analysis = self.xml_agent.analyze_document(file_path)\n",
    "                strategy = self.xml_agent.suggest_processing_strategy(analysis)\n",
    "                response = f\"Based on my analysis, I recommend the '{strategy['strategy']}' approach. Here's why:\\n\\n\"\n",
    "                response += f\"**Recommended Tools:** {', '.join(strategy['recommended_tools'])}\\n\"\n",
    "                response += f\"**Best Chunking:** {strategy['chunking']}\\n\"\n",
    "                response += f\"**AI Applications:** {', '.join(strategy['ai_applications'])}\"\n",
    "            else:\n",
    "                response = \"Please provide an XML file to get processing recommendations.\"\n",
    "                \n",
    "        elif \"chunk\" in query_lower:\n",
    "            if file_path:\n",
    "                chunks = xaf.chunk(file_path, strategy=\"auto\")\n",
    "                response = f\"I've created {len(chunks)} optimized chunks from your document. \"\n",
    "                response += f\"Total estimated tokens: {sum(c.token_estimate for c in chunks)}. \"\n",
    "                response += \"These chunks are ready for vector database ingestion or LLM processing.\"\n",
    "            else:\n",
    "                response = \"Please provide an XML file to chunk.\"\n",
    "                \n",
    "        elif \"ai use case\" in query_lower or \"applications\" in query_lower:\n",
    "            if file_path:\n",
    "                analysis = self.xml_agent.analyze_document(file_path)\n",
    "                use_cases = analysis['ai_insights']['use_cases']\n",
    "                response = f\"This {analysis['document_info']['document_type']} document is well-suited for:\\n\\n\"\n",
    "                for i, use_case in enumerate(use_cases, 1):\n",
    "                    response += f\"{i}. {use_case}\\n\"\n",
    "            else:\n",
    "                response = \"Please provide an XML file to identify AI use cases.\"\n",
    "                \n",
    "        else:\n",
    "            response = \"I can help you analyze XML documents, recommend processing strategies, create chunks, and identify AI use cases. What would you like to do?\"\n",
    "        \n",
    "        # Add response to history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Initialize simulated agent\n",
    "agent = SimulatedLangChainAgent(xml_agent)\n",
    "print(\"ðŸ¤– Simulated LangChain Agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Agent Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different queries\n",
    "queries = [\n",
    "    \"What can you tell me about this XML document?\",\n",
    "    \"What processing strategy do you recommend?\",\n",
    "    \"How should I chunk this document?\",\n",
    "    \"What are the AI use cases for this data?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ—£ï¸ **Agent Conversation Demo:**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n**User:** {query}\")\n",
    "    response = agent.process_user_query(query, test_file)\n",
    "    print(f\"**Agent:** {response}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for Vector Database\n",
    "\n",
    "Prepare the analyzed data for vector database ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_vector_db(analysis, chunks):\n",
    "    \"\"\"Prepare XML analysis results for vector database ingestion\"\"\"\n",
    "    \n",
    "    vector_docs = []\n",
    "    \n",
    "    # Document-level metadata\n",
    "    doc_metadata = {\n",
    "        'document_type': analysis['document_info']['document_type'],\n",
    "        'confidence': analysis['document_info']['confidence'],\n",
    "        'handler_used': analysis['document_info']['handler_used'],\n",
    "        'total_elements': analysis['schema_info']['total_elements'],\n",
    "        'max_depth': analysis['schema_info']['max_depth'],\n",
    "        'ai_use_cases': analysis['ai_insights']['use_cases'],\n",
    "        'processing_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Create vector documents for each chunk\n",
    "    for chunk in chunks:\n",
    "        vector_doc = {\n",
    "            'id': f\"{analysis['document_info']['file_path']}_{chunk.chunk_id}\",\n",
    "            'content': chunk.content,\n",
    "            'metadata': {\n",
    "                **doc_metadata,\n",
    "                'chunk_id': chunk.chunk_id,\n",
    "                'element_path': chunk.element_path,\n",
    "                'start_line': chunk.start_line,\n",
    "                'end_line': chunk.end_line,\n",
    "                'elements_included': chunk.elements_included,\n",
    "                'token_estimate': chunk.token_estimate,\n",
    "                'chunk_metadata': chunk.metadata\n",
    "            }\n",
    "        }\n",
    "        vector_docs.append(vector_doc)\n",
    "    \n",
    "    return vector_docs\n",
    "\n",
    "# Get all chunks for vector DB preparation\n",
    "all_chunks = xaf.chunk(test_file, strategy=\"hierarchical\")\n",
    "vector_docs = prepare_for_vector_db(analysis, all_chunks)\n",
    "\n",
    "print(f\"ðŸ“Š Prepared {len(vector_docs)} documents for vector database ingestion\")\n",
    "print(f\"Sample document structure:\")\n",
    "print(json.dumps(vector_docs[0], indent=2, default=str)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real LangChain Integration Template\n",
    "\n",
    "Here's a template for actual LangChain integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for real LangChain integration\n",
    "langchain_integration_template = '''\n",
    "# Real LangChain Integration Template\n",
    "# (Uncomment and modify when you have API keys)\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Set up your API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Initialize components\n",
    "llm = OpenAI(temperature=0.1)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create XML analysis tool\n",
    "def xml_analysis_tool(file_path: str) -> str:\n",
    "    \"\"\"Analyze XML document and return insights\"\"\"\n",
    "    agent = XMLAnalysisAgent()\n",
    "    analysis = agent.analyze_document(file_path)\n",
    "    return agent.generate_summary(analysis)\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"XML Document Analyzer\",\n",
    "        func=xml_analysis_tool,\n",
    "        description=\"Analyze XML documents to understand structure, content, and AI applications\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Use the agent\n",
    "response = agent.run(\"Please analyze the XML document and suggest how to use it for AI applications\")\n",
    "'''\n",
    "\n",
    "print(\"ðŸ“‹ LangChain Integration Template:\")\n",
    "print(langchain_integration_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates how the XML Analysis Framework can be integrated into agentic AI workflows:\n",
    "\n",
    "1. âœ… **Smart Document Analysis**: Automatically detect document types and extract insights\n",
    "2. âœ… **Intelligent Strategy Recommendations**: AI-driven suggestions for processing approaches\n",
    "3. âœ… **Vector Database Preparation**: Ready-to-use format for embeddings and search\n",
    "4. âœ… **Conversational Interface**: Natural language interaction with XML analysis\n",
    "5. âœ… **LangChain Integration**: Template for real agent implementations\n",
    "\n",
    "**Next Steps:**\n",
    "- Add your API keys to enable real LLM integration\n",
    "- Customize the agent logic for your specific use cases\n",
    "- Integrate with vector databases like Pinecone, Weaviate, or Chroma\n",
    "- Add more sophisticated reasoning and planning capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}