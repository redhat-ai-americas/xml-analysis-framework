{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Stage 1: Data Ingestion and Analysis\n",
    "\n",
    "This notebook is designed to be part of an Elyra pipeline. It handles:\n",
    "1. Loading multiple XML documents\n",
    "2. Analyzing each document with the XML Analysis Framework\n",
    "3. Preparing data for downstream processing\n",
    "4. Saving results for the next pipeline stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T02:44:03.633047Z",
     "iopub.status.busy": "2025-07-28T02:44:03.632748Z",
     "iopub.status.idle": "2025-07-28T02:44:15.672162Z",
     "shell.execute_reply": "2025-07-28T02:44:15.671847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "streamlit 1.45.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\r\n",
      "gradio 5.29.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.104.1 which is incompatible.\r\n",
      "gradio 5.29.0 requires starlette<1.0,>=0.40.0; sys_platform != \"emscripten\", but you have starlette 0.27.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML Analysis Framework version: 1.2.12\n",
      "Processing started at: 2025-07-27 21:44:15.670630\n"
     ]
    }
   ],
   "source": [
    "# Install required packages in the pipeline environment\n",
    "%pip install xml-analysis-framework==1.2.12 --upgrade -q --force-reinstall --no-cache-dir\n",
    "%pip install pandas --upgrade -q --force-reinstall --no-cache-dir\n",
    "\n",
    "import xml_analysis_framework as xaf\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"XML Analysis Framework version: {xaf.__version__}\")\n",
    "print(f\"Processing started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T02:44:15.673907Z",
     "iopub.status.busy": "2025-07-28T02:44:15.673762Z",
     "iopub.status.idle": "2025-07-28T02:44:15.676759Z",
     "shell.execute_reply": "2025-07-28T02:44:15.676486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 XML files to process:\n",
      "  - full_export.xml (8.0 KB)\n",
      "  - spring-boot-example-pom.xml (2.1 KB)\n",
      "  - ivysettings.xml (3.3 KB)\n",
      "  - build.xml (16.1 KB)\n",
      "  - ivy.xml (4.3 KB)\n",
      "  - mapbox-example.kml (1.1 KB)\n"
     ]
    }
   ],
   "source": [
    "# Configure input data directory\n",
    "DATA_DIR = Path(\"data\")\n",
    "OUTPUT_DIR = Path(\"pipeline_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Find all XML files to process\n",
    "xml_files = list(DATA_DIR.glob(\"*.xml\")) + list(DATA_DIR.glob(\"*.kml\"))\n",
    "\n",
    "print(f\"Found {len(xml_files)} XML files to process:\")\n",
    "for file in xml_files:\n",
    "    print(f\"  - {file.name} ({file.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "if not xml_files:\n",
    "    print(\"⚠️ No XML files found. Adding sample file...\")\n",
    "    xml_files = [DATA_DIR / \"mapbox-example.kml\"]\n",
    "    print(f\"Using sample file: {xml_files[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Document Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T02:44:15.678239Z",
     "iopub.status.busy": "2025-07-28T02:44:15.678136Z",
     "iopub.status.idle": "2025-07-28T02:44:15.702494Z",
     "shell.execute_reply": "2025-07-28T02:44:15.702269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing documents...\n",
      "\n",
      "[1/6] Processing: full_export.xml\n",
      "File size: 0.0 MB\n",
      "Using iterative parsing for large file: data/full_export.xml\n",
      "  ✅ Success: ServiceNow Incident (4 chunks)\n",
      "\n",
      "[2/6] Processing: spring-boot-example-pom.xml\n",
      "File size: 0.0 MB\n",
      "Using iterative parsing for large file: data/spring-boot-example-pom.xml\n",
      "  ✅ Success: Maven POM (6 chunks)\n",
      "\n",
      "[3/6] Processing: ivysettings.xml\n",
      "File size: 0.0 MB\n",
      "Using iterative parsing for large file: data/ivysettings.xml\n",
      "  ✅ Success: Ivy Settings (0 chunks)\n",
      "\n",
      "[4/6] Processing: build.xml\n",
      "File size: 0.0 MB\n",
      "Using iterative parsing for large file: data/build.xml\n",
      "  ✅ Success: Apache Ant Build (4 chunks)\n",
      "\n",
      "[5/6] Processing: ivy.xml\n",
      "File size: 0.0 MB\n",
      "Using iterative parsing for large file: data/ivy.xml\n",
      "  ✅ Success: Ivy Module Descriptor (0 chunks)\n",
      "\n",
      "[6/6] Processing: mapbox-example.kml\n",
      "File size: 0.0 MB\n",
      "Using iterative parsing for large file: data/mapbox-example.kml\n",
      "  ✅ Success: KML Geographic Data (0 chunks)\n",
      "\n",
      "✅ Processed 6 documents\n"
     ]
    }
   ],
   "source": [
    "def analyze_document_for_pipeline(file_path: Path) -> dict:\n",
    "    \"\"\"Analyze a single document and prepare for pipeline processing\"\"\"\n",
    "    try:\n",
    "        # Comprehensive analysis\n",
    "        result = xaf.analyze(str(file_path))\n",
    "        enhanced = xaf.analyze_enhanced(str(file_path))\n",
    "        schema = xaf.analyze_schema(str(file_path))\n",
    "        \n",
    "        # Create chunks for vector processing\n",
    "        chunks = xaf.chunk(str(file_path), strategy=\"hierarchical\")\n",
    "        \n",
    "        # Compile pipeline-ready data\n",
    "        return {\n",
    "            'file_info': {\n",
    "                'file_name': file_path.name,\n",
    "                'file_path': str(file_path),\n",
    "                'file_size': f\"{file_path.stat().st_size:,} bytes ({file_path.stat().st_size / 1024:.1f} KB)\",\n",
    "                'processed_at': datetime.now().isoformat()\n",
    "            },\n",
    "            'document_analysis': {\n",
    "                'document_type': result['document_type'].type_name,\n",
    "                'confidence': result['document_type'].confidence,\n",
    "                'handler_used': result['handler_used'],\n",
    "                'ai_use_cases': enhanced.ai_use_cases,\n",
    "                'key_findings': enhanced.key_findings,\n",
    "                'quality_metrics': enhanced.quality_metrics or {},\n",
    "                'structured_data': enhanced.structured_data\n",
    "            },\n",
    "            'schema_info': {\n",
    "                'total_elements': schema.total_elements,\n",
    "                'max_depth': schema.max_depth,\n",
    "                'root_element': schema.root_element,\n",
    "                'namespaces': schema.namespaces\n",
    "            },\n",
    "            'chunks': [\n",
    "                {\n",
    "                    'chunk_id': chunk.chunk_id,\n",
    "                    'content': chunk.content,\n",
    "                    'element_path': chunk.element_path,\n",
    "                    'start_line': chunk.start_line,\n",
    "                    'end_line': chunk.end_line,\n",
    "                    'elements_included': chunk.elements_included,\n",
    "                    'metadata': chunk.metadata,\n",
    "                    'token_estimate': chunk.token_estimate\n",
    "                }\n",
    "                for chunk in chunks\n",
    "            ],\n",
    "            'processing_stats': {\n",
    "                'total_chunks': len(chunks),\n",
    "                'total_tokens': sum(chunk.token_estimate for chunk in chunks),\n",
    "                'avg_chunk_size': sum(len(chunk.content) for chunk in chunks) / len(chunks) if chunks else 0\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'file_info': {\n",
    "                'file_name': file_path.name,\n",
    "                'file_path': str(file_path),\n",
    "                'file_size': f\"{file_path.stat().st_size:,} bytes ({file_path.stat().st_size / 1024:.1f} KB)\",\n",
    "                'processed_at': datetime.now().isoformat()\n",
    "            },\n",
    "            'error': str(e),\n",
    "            'processing_stats': {'total_chunks': 0, 'total_tokens': 0, 'avg_chunk_size': 0}\n",
    "        }\n",
    "\n",
    "# Process all documents\n",
    "print(\"🔄 Processing documents...\")\n",
    "processed_documents = []\n",
    "processing_summary = []\n",
    "\n",
    "for i, file_path in enumerate(xml_files, 1):\n",
    "    print(f\"\\n[{i}/{len(xml_files)}] Processing: {file_path.name}\")\n",
    "    \n",
    "    doc_analysis = analyze_document_for_pipeline(file_path)\n",
    "    processed_documents.append(doc_analysis)\n",
    "    \n",
    "    # Create summary entry\n",
    "    if 'error' not in doc_analysis:\n",
    "        summary_entry = {\n",
    "            'file_name': doc_analysis['file_info']['file_name'],\n",
    "            'document_type': doc_analysis['document_analysis']['document_type'],\n",
    "            'confidence': doc_analysis['document_analysis']['confidence'],\n",
    "            'total_chunks': doc_analysis['processing_stats']['total_chunks'],\n",
    "            'total_tokens': doc_analysis['processing_stats']['total_tokens'],\n",
    "            'status': 'success'\n",
    "        }\n",
    "        print(f\"  ✅ Success: {summary_entry['document_type']} ({summary_entry['total_chunks']} chunks)\")\n",
    "    else:\n",
    "        summary_entry = {\n",
    "            'file_name': doc_analysis['file_info']['file_name'],\n",
    "            'document_type': 'unknown',\n",
    "            'confidence': 0.0,\n",
    "            'total_chunks': 0,\n",
    "            'total_tokens': 0,\n",
    "            'status': 'error',\n",
    "            'error': doc_analysis['error']\n",
    "        }\n",
    "        print(f\"  ❌ Error: {summary_entry['error']}\")\n",
    "    \n",
    "    processing_summary.append(summary_entry)\n",
    "\n",
    "print(f\"\\n✅ Processed {len(processed_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T02:44:15.703907Z",
     "iopub.status.busy": "2025-07-28T02:44:15.703818Z",
     "iopub.status.idle": "2025-07-28T02:44:15.709316Z",
     "shell.execute_reply": "2025-07-28T02:44:15.709072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Processing Summary:\n",
      "                     file_name          document_type  confidence  \\\n",
      "0              full_export.xml    ServiceNow Incident        0.95   \n",
      "1  spring-boot-example-pom.xml              Maven POM        1.00   \n",
      "2              ivysettings.xml           Ivy Settings        0.95   \n",
      "3                    build.xml       Apache Ant Build        0.95   \n",
      "4                      ivy.xml  Ivy Module Descriptor        0.95   \n",
      "5           mapbox-example.kml    KML Geographic Data        0.95   \n",
      "\n",
      "   total_chunks  total_tokens   status  \n",
      "0             4             4  success  \n",
      "1             6            39  success  \n",
      "2             0             0  success  \n",
      "3             4            23  success  \n",
      "4             0             0  success  \n",
      "5             0             0  success  \n",
      "\n",
      "📈 Overall Statistics:\n",
      "  • Successful analyses: 6/6\n",
      "  • Total chunks generated: 14\n",
      "  • Total tokens estimated: 66\n",
      "  • Document types found: 6\n",
      "\n",
      "📋 Document Types:\n",
      "  • ServiceNow Incident: 1 document(s)\n",
      "  • Maven POM: 1 document(s)\n",
      "  • Ivy Settings: 1 document(s)\n",
      "  • Apache Ant Build: 1 document(s)\n",
      "  • Ivy Module Descriptor: 1 document(s)\n",
      "  • KML Geographic Data: 1 document(s)\n"
     ]
    }
   ],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(processing_summary)\n",
    "\n",
    "print(\"📊 Processing Summary:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Overall statistics\n",
    "successful_docs = summary_df[summary_df['status'] == 'success']\n",
    "total_chunks = successful_docs['total_chunks'].sum()\n",
    "total_tokens = successful_docs['total_tokens'].sum()\n",
    "\n",
    "print(f\"\\n📈 Overall Statistics:\")\n",
    "print(f\"  • Successful analyses: {len(successful_docs)}/{len(summary_df)}\")\n",
    "print(f\"  • Total chunks generated: {total_chunks:,}\")\n",
    "print(f\"  • Total tokens estimated: {total_tokens:,}\")\n",
    "print(f\"  • Document types found: {successful_docs['document_type'].nunique()}\")\n",
    "\n",
    "# Document type distribution\n",
    "if len(successful_docs) > 0:\n",
    "    print(f\"\\n📋 Document Types:\")\n",
    "    type_counts = successful_docs['document_type'].value_counts()\n",
    "    for doc_type, count in type_counts.items():\n",
    "        print(f\"  • {doc_type}: {count} document(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pipeline Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T02:44:15.710687Z",
     "iopub.status.busy": "2025-07-28T02:44:15.710604Z",
     "iopub.status.idle": "2025-07-28T02:44:15.720539Z",
     "shell.execute_reply": "2025-07-28T02:44:15.720210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved detailed analyses to: pipeline_outputs/document_analyses.json\n",
      "💾 Saved processing summary to: pipeline_outputs/processing_summary.csv\n",
      "💾 Saved 14 vector-ready chunks to: pipeline_outputs/vector_ready_chunks.json\n",
      "💾 Saved pipeline metadata to: pipeline_outputs/pipeline_metadata.json\n",
      "\n",
      "🎉 Data ingestion stage completed successfully!\n",
      "Ready for next pipeline stage: Vector Database Population\n"
     ]
    }
   ],
   "source": [
    "# Save complete analysis results\n",
    "analysis_output_file = OUTPUT_DIR / \"document_analyses.json\"\n",
    "with open(analysis_output_file, 'w') as f:\n",
    "    json.dump(processed_documents, f, indent=2, default=str)\n",
    "    \n",
    "print(f\"💾 Saved detailed analyses to: {analysis_output_file}\")\n",
    "\n",
    "# Save processing summary\n",
    "summary_output_file = OUTPUT_DIR / \"processing_summary.csv\"\n",
    "summary_df.to_csv(summary_output_file, index=False)\n",
    "print(f\"💾 Saved processing summary to: {summary_output_file}\")\n",
    "\n",
    "# Prepare vector-ready chunks for next stage\n",
    "vector_chunks = []\n",
    "for doc in processed_documents:\n",
    "    if 'error' not in doc:\n",
    "        for chunk in doc['chunks']:\n",
    "            vector_chunk = {\n",
    "                'id': f\"{doc['file_info']['file_name']}_{chunk['chunk_id']}\",\n",
    "                'content': chunk['content'],\n",
    "                'metadata': {\n",
    "                    'source_file': doc['file_info']['file_name'],\n",
    "                    'document_type': doc['document_analysis']['document_type'],\n",
    "                    'confidence': doc['document_analysis']['confidence'],\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'element_path': chunk['element_path'],\n",
    "                    'token_estimate': chunk['token_estimate'],\n",
    "                    'ai_use_cases': doc['document_analysis']['ai_use_cases']\n",
    "                }\n",
    "            }\n",
    "            vector_chunks.append(vector_chunk)\n",
    "\n",
    "vector_output_file = OUTPUT_DIR / \"vector_ready_chunks.json\"\n",
    "with open(vector_output_file, 'w') as f:\n",
    "    json.dump(vector_chunks, f, indent=2)\n",
    "\n",
    "print(f\"💾 Saved {len(vector_chunks)} vector-ready chunks to: {vector_output_file}\")\n",
    "\n",
    "# Helper function to convert pandas/numpy types to JSON-serializable types\n",
    "def convert_for_json(obj):\n",
    "    \"\"\"Convert pandas/numpy types to JSON-serializable types\"\"\"\n",
    "    if hasattr(obj, 'item'):  # numpy/pandas scalar\n",
    "        return obj.item()\n",
    "    elif hasattr(obj, 'tolist'):  # numpy array\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Create pipeline metadata (with type conversion)\n",
    "pipeline_metadata = {\n",
    "    'pipeline_stage': 'data_ingestion',\n",
    "    'processed_at': datetime.now().isoformat(),\n",
    "    'input_files': [str(f) for f in xml_files],\n",
    "    'total_documents': len(processed_documents),\n",
    "    'successful_documents': len(successful_docs),\n",
    "    'total_chunks': convert_for_json(total_chunks),  # Convert pandas int64\n",
    "    'total_tokens': convert_for_json(total_tokens),   # Convert pandas int64\n",
    "    'output_files': {\n",
    "        'analyses': str(analysis_output_file),\n",
    "        'summary': str(summary_output_file),\n",
    "        'vector_chunks': str(vector_output_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_file = OUTPUT_DIR / \"pipeline_metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(pipeline_metadata, f, indent=2)\n",
    "\n",
    "print(f\"💾 Saved pipeline metadata to: {metadata_file}\")\n",
    "print(f\"\\n🎉 Data ingestion stage completed successfully!\")\n",
    "print(f\"Ready for next pipeline stage: Vector Database Population\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
