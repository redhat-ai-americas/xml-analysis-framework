{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Stage 1: Data Ingestion and Analysis\n",
    "\n",
    "This notebook is designed to be part of an Elyra pipeline. It handles:\n",
    "1. Loading multiple XML documents\n",
    "2. Analyzing each document with the XML Analysis Framework\n",
    "3. Preparing data for downstream processing\n",
    "4. Saving results for the next pipeline stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages in the pipeline environment\n",
    "!pip install xml-analysis-framework\n",
    "!pip install pandas\n",
    "\n",
    "import xml_analysis_framework as xaf\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"XML Analysis Framework version: {xaf.__version__}\")\n",
    "print(f\"Processing started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure input data directory\n",
    "DATA_DIR = Path(\"data\")\n",
    "OUTPUT_DIR = Path(\"pipeline_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Find all XML files to process\n",
    "xml_files = list(DATA_DIR.glob(\"*.xml\")) + list(DATA_DIR.glob(\"*.kml\"))\n",
    "\n",
    "print(f\"Found {len(xml_files)} XML files to process:\")\n",
    "for file in xml_files:\n",
    "    print(f\"  - {file.name} ({file.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "if not xml_files:\n",
    "    print(\"âš ï¸ No XML files found. Adding sample file...\")\n",
    "    xml_files = [DATA_DIR / \"mapbox-example.kml\"]\n",
    "    print(f\"Using sample file: {xml_files[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Document Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document_for_pipeline(file_path: Path) -> dict:\n",
    "    \"\"\"Analyze a single document and prepare for pipeline processing\"\"\"\n",
    "    try:\n",
    "        # Comprehensive analysis\n",
    "        result = xaf.analyze(str(file_path))\n",
    "        enhanced = xaf.analyze_enhanced(str(file_path))\n",
    "        schema = xaf.analyze_schema(str(file_path))\n",
    "        \n",
    "        # Create chunks for vector processing\n",
    "        chunks = xaf.chunk(str(file_path), strategy=\"hierarchical\")\n",
    "        \n",
    "        # Compile pipeline-ready data\n",
    "        return {\n",
    "            'file_info': {\n",
    "                'file_name': file_path.name,\n",
    "                'file_path': str(file_path),\n",
    "                'file_size': file_path.stat().st_size,\n",
    "                'processed_at': datetime.now().isoformat()\n",
    "            },\n",
    "            'document_analysis': {\n",
    "                'document_type': result['document_type'].type_name,\n",
    "                'confidence': result['document_type'].confidence,\n",
    "                'handler_used': result['handler_used'],\n",
    "                'ai_use_cases': enhanced.ai_use_cases,\n",
    "                'key_findings': enhanced.key_findings,\n",
    "                'quality_metrics': enhanced.quality_metrics or {},\n",
    "                'structured_data': enhanced.structured_data\n",
    "            },\n",
    "            'schema_info': {\n",
    "                'total_elements': schema.total_elements,\n",
    "                'max_depth': schema.max_depth,\n",
    "                'root_element': schema.root_element,\n",
    "                'namespaces': schema.namespaces\n",
    "            },\n",
    "            'chunks': [\n",
    "                {\n",
    "                    'chunk_id': chunk.chunk_id,\n",
    "                    'content': chunk.content,\n",
    "                    'element_path': chunk.element_path,\n",
    "                    'start_line': chunk.start_line,\n",
    "                    'end_line': chunk.end_line,\n",
    "                    'elements_included': chunk.elements_included,\n",
    "                    'metadata': chunk.metadata,\n",
    "                    'token_estimate': chunk.token_estimate\n",
    "                }\n",
    "                for chunk in chunks\n",
    "            ],\n",
    "            'processing_stats': {\n",
    "                'total_chunks': len(chunks),\n",
    "                'total_tokens': sum(chunk.token_estimate for chunk in chunks),\n",
    "                'avg_chunk_size': sum(len(chunk.content) for chunk in chunks) / len(chunks) if chunks else 0\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'file_info': {\n",
    "                'file_name': file_path.name,\n",
    "                'file_path': str(file_path),\n",
    "                'file_size': file_path.stat().st_size,\n",
    "                'processed_at': datetime.now().isoformat()\n",
    "            },\n",
    "            'error': str(e),\n",
    "            'processing_stats': {'total_chunks': 0, 'total_tokens': 0, 'avg_chunk_size': 0}\n",
    "        }\n",
    "\n",
    "# Process all documents\n",
    "print(\"ðŸ”„ Processing documents...\")\n",
    "processed_documents = []\n",
    "processing_summary = []\n",
    "\n",
    "for i, file_path in enumerate(xml_files, 1):\n",
    "    print(f\"\\n[{i}/{len(xml_files)}] Processing: {file_path.name}\")\n",
    "    \n",
    "    doc_analysis = analyze_document_for_pipeline(file_path)\n",
    "    processed_documents.append(doc_analysis)\n",
    "    \n",
    "    # Create summary entry\n",
    "    if 'error' not in doc_analysis:\n",
    "        summary_entry = {\n",
    "            'file_name': doc_analysis['file_info']['file_name'],\n",
    "            'document_type': doc_analysis['document_analysis']['document_type'],\n",
    "            'confidence': doc_analysis['document_analysis']['confidence'],\n",
    "            'total_chunks': doc_analysis['processing_stats']['total_chunks'],\n",
    "            'total_tokens': doc_analysis['processing_stats']['total_tokens'],\n",
    "            'status': 'success'\n",
    "        }\n",
    "        print(f\"  âœ… Success: {summary_entry['document_type']} ({summary_entry['total_chunks']} chunks)\")\n",
    "    else:\n",
    "        summary_entry = {\n",
    "            'file_name': doc_analysis['file_info']['file_name'],\n",
    "            'document_type': 'unknown',\n",
    "            'confidence': 0.0,\n",
    "            'total_chunks': 0,\n",
    "            'total_tokens': 0,\n",
    "            'status': 'error',\n",
    "            'error': doc_analysis['error']\n",
    "        }\n",
    "        print(f\"  âŒ Error: {summary_entry['error']}\")\n",
    "    \n",
    "    processing_summary.append(summary_entry)\n",
    "\n",
    "print(f\"\\nâœ… Processed {len(processed_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(processing_summary)\n",
    "\n",
    "print(\"ðŸ“Š Processing Summary:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Overall statistics\n",
    "successful_docs = summary_df[summary_df['status'] == 'success']\n",
    "total_chunks = successful_docs['total_chunks'].sum()\n",
    "total_tokens = successful_docs['total_tokens'].sum()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Overall Statistics:\")\n",
    "print(f\"  â€¢ Successful analyses: {len(successful_docs)}/{len(summary_df)}\")\n",
    "print(f\"  â€¢ Total chunks generated: {total_chunks:,}\")\n",
    "print(f\"  â€¢ Total tokens estimated: {total_tokens:,}\")\n",
    "print(f\"  â€¢ Document types found: {successful_docs['document_type'].nunique()}\")\n",
    "\n",
    "# Document type distribution\n",
    "if len(successful_docs) > 0:\n",
    "    print(f\"\\nðŸ“‹ Document Types:\")\n",
    "    type_counts = successful_docs['document_type'].value_counts()\n",
    "    for doc_type, count in type_counts.items():\n",
    "        print(f\"  â€¢ {doc_type}: {count} document(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pipeline Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete analysis results\n",
    "analysis_output_file = OUTPUT_DIR / \"document_analyses.json\"\n",
    "with open(analysis_output_file, 'w') as f:\n",
    "    json.dump(processed_documents, f, indent=2, default=str)\n",
    "    \n",
    "print(f\"ðŸ’¾ Saved detailed analyses to: {analysis_output_file}\")\n",
    "\n",
    "# Save processing summary\n",
    "summary_output_file = OUTPUT_DIR / \"processing_summary.csv\"\n",
    "summary_df.to_csv(summary_output_file, index=False)\n",
    "print(f\"ðŸ’¾ Saved processing summary to: {summary_output_file}\")\n",
    "\n",
    "# Prepare vector-ready chunks for next stage\n",
    "vector_chunks = []\n",
    "for doc in processed_documents:\n",
    "    if 'error' not in doc:\n",
    "        for chunk in doc['chunks']:\n",
    "            vector_chunk = {\n",
    "                'id': f\"{doc['file_info']['file_name']}_{chunk['chunk_id']}\",\n",
    "                'content': chunk['content'],\n",
    "                'metadata': {\n",
    "                    'source_file': doc['file_info']['file_name'],\n",
    "                    'document_type': doc['document_analysis']['document_type'],\n",
    "                    'confidence': doc['document_analysis']['confidence'],\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'element_path': chunk['element_path'],\n",
    "                    'token_estimate': chunk['token_estimate'],\n",
    "                    'ai_use_cases': doc['document_analysis']['ai_use_cases']\n",
    "                }\n",
    "            }\n",
    "            vector_chunks.append(vector_chunk)\n",
    "\n",
    "vector_output_file = OUTPUT_DIR / \"vector_ready_chunks.json\"\n",
    "with open(vector_output_file, 'w') as f:\n",
    "    json.dump(vector_chunks, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved {len(vector_chunks)} vector-ready chunks to: {vector_output_file}\")\n",
    "\n",
    "# Create pipeline metadata\n",
    "pipeline_metadata = {\n",
    "    'pipeline_stage': 'data_ingestion',\n",
    "    'processed_at': datetime.now().isoformat(),\n",
    "    'input_files': [str(f) for f in xml_files],\n",
    "    'total_documents': len(processed_documents),\n",
    "    'successful_documents': len(successful_docs),\n",
    "    'total_chunks': total_chunks,\n",
    "    'total_tokens': total_tokens,\n",
    "    'output_files': {\n",
    "        'analyses': str(analysis_output_file),\n",
    "        'summary': str(summary_output_file),\n",
    "        'vector_chunks': str(vector_output_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_file = OUTPUT_DIR / \"pipeline_metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(pipeline_metadata, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved pipeline metadata to: {metadata_file}\")\n",
    "print(f\"\\nðŸŽ‰ Data ingestion stage completed successfully!\")\n",
    "print(f\"Ready for next pipeline stage: Vector Database Population\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}