{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML Analysis Framework - Documentation Testing\n",
    "\n",
    "This notebook contains all the examples from the README documentation, allowing you to test them interactively and verify they work correctly.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the XML analysis framework\n",
    "!pip install xml-analysis-framework\n",
    "\n",
    "# Import required modules\n",
    "import xml_analysis_framework as xaf\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"XML Analysis Framework version: {xaf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "We'll use a synthetic KML file for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to our test file\n",
    "test_file = \"data/mapbox-example.kml\"\n",
    "\n",
    "# Verify the file exists\n",
    "if Path(test_file).exists():\n",
    "    print(f\"‚úÖ Test file found: {test_file}\")\n",
    "    print(f\"File size: {Path(test_file).stat().st_size / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(f\"‚ùå Test file not found: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple API Examples\n",
    "\n",
    "Testing the simple API from the README:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ One-line analysis with specialized handlers\n",
    "result = xaf.analyze(test_file)\n",
    "print(f\"Document type: {result['document_type'].type_name}\")\n",
    "print(f\"Handler used: {result['handler_used']}\")\n",
    "print(f\"Confidence: {result['document_type'].confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Basic schema analysis  \n",
    "schema = xaf.analyze_schema(test_file)\n",
    "print(f\"Elements: {schema.total_elements}, Depth: {schema.max_depth}\")\n",
    "print(f\"Root element: {schema.root_element}\")\n",
    "print(f\"Namespaces: {schema.namespaces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÇÔ∏è Smart chunking for AI/ML\n",
    "chunks = xaf.chunk(test_file, strategy=\"auto\")\n",
    "print(f\"Created {len(chunks)} optimized chunks\")\n",
    "\n",
    "# Show details of first few chunks\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  ID: {chunk.chunk_id}\")\n",
    "    print(f\"  Content length: {len(chunk.content)} chars\")\n",
    "    print(f\"  Element path: {chunk.element_path}\")\n",
    "    print(f\"  Token estimate: {chunk.token_estimate}\")\n",
    "    print(f\"  Elements included: {chunk.elements_included}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save chunks to JSON\n",
    "chunks_data = [\n",
    "    {\n",
    "        \"chunk_id\": chunk.chunk_id,\n",
    "        \"content\": chunk.content,\n",
    "        \"element_path\": chunk.element_path,\n",
    "        \"start_line\": chunk.start_line,\n",
    "        \"end_line\": chunk.end_line,\n",
    "        \"elements_included\": chunk.elements_included,\n",
    "        \"metadata\": chunk.metadata,\n",
    "        \"token_estimate\": chunk.token_estimate\n",
    "    }\n",
    "    for chunk in chunks\n",
    "]\n",
    "\n",
    "# Write to file\n",
    "with open(\"chunks_output.json\", \"w\") as f:\n",
    "    json.dump(chunks_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(chunks_data)} chunks to chunks_output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced API Examples\n",
    "\n",
    "Testing the advanced API with multiple chunking strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced analysis with full results\n",
    "analysis = xaf.analyze_enhanced(test_file)\n",
    "\n",
    "print(f\"Type: {analysis.type_name} (confidence: {analysis.confidence:.2f})\")\n",
    "print(f\"AI use cases: {len(analysis.ai_use_cases)}\")\n",
    "print(f\"AI use cases: {analysis.ai_use_cases}\")\n",
    "\n",
    "if analysis.quality_metrics:\n",
    "    print(f\"Quality metrics: {analysis.quality_metrics}\")\n",
    "else:\n",
    "    print(\"Quality metrics: Not available\")\n",
    "\n",
    "print(f\"\\nKey findings: {analysis.key_findings}\")\n",
    "print(f\"Structured data keys: {list(analysis.structured_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different chunking strategies\n",
    "hierarchical_chunks = xaf.chunk(test_file, strategy=\"hierarchical\")\n",
    "sliding_chunks = xaf.chunk(test_file, strategy=\"sliding_window\") \n",
    "content_chunks = xaf.chunk(test_file, strategy=\"content_aware\")\n",
    "\n",
    "print(f\"Hierarchical chunks: {len(hierarchical_chunks)}\")\n",
    "print(f\"Sliding window chunks: {len(sliding_chunks)}\")\n",
    "print(f\"Content-aware chunks: {len(content_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process chunks\n",
    "print(\"First 3 hierarchical chunks:\")\n",
    "for chunk in hierarchical_chunks[:3]:\n",
    "    print(f\"Chunk {chunk.chunk_id}: {len(chunk.content)} chars\")\n",
    "    print(f\"Path: {chunk.element_path}, Elements: {len(chunk.elements_included)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save different chunking strategies to separate files\n",
    "# Helper function to convert chunk to dict\n",
    "def chunk_to_dict(chunk):\n",
    "    return {\n",
    "        \"chunk_id\": chunk.chunk_id,\n",
    "        \"content\": chunk.content,\n",
    "        \"element_path\": chunk.element_path,\n",
    "        \"start_line\": chunk.start_line,\n",
    "        \"end_line\": chunk.end_line,\n",
    "        \"elements_included\": chunk.elements_included,\n",
    "        \"metadata\": chunk.metadata,\n",
    "        \"token_estimate\": chunk.token_estimate\n",
    "    }\n",
    "\n",
    "# Save each strategy's results\n",
    "strategies = {\n",
    "    \"hierarchical\": hierarchical_chunks,\n",
    "    \"sliding_window\": sliding_chunks,\n",
    "    \"content_aware\": content_chunks\n",
    "}\n",
    "\n",
    "for strategy_name, chunks in strategies.items():\n",
    "    chunks_data = [chunk_to_dict(chunk) for chunk in chunks]\n",
    "    \n",
    "    with open(f\"chunks_{strategy_name}.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"strategy\": strategy_name,\n",
    "            \"total_chunks\": len(chunks_data),\n",
    "            \"chunks\": chunks_data\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(chunks_data)} chunks to chunks_{strategy_name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert API Examples\n",
    "\n",
    "Testing the expert API with direct class access and custom configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For advanced customization, use the classes directly\n",
    "from xml_analysis_framework import XMLDocumentAnalyzer, ChunkingOrchestrator\n",
    "\n",
    "analyzer = XMLDocumentAnalyzer(max_file_size_mb=500)\n",
    "orchestrator = ChunkingOrchestrator(max_file_size_mb=1000)\n",
    "\n",
    "# Custom analysis\n",
    "result = analyzer.analyze_document(test_file)\n",
    "\n",
    "print(f\"Analysis result type: {type(result)}\")\n",
    "print(f\"Document type: {result.type_name}\")\n",
    "print(f\"Confidence: {result.confidence}\")\n",
    "print(f\"Handler used: {result.handler_used}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom chunking with config\n",
    "from xml_analysis_framework.core.chunking import ChunkingConfig\n",
    "\n",
    "config = ChunkingConfig(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=300,\n",
    "    overlap_size=150,\n",
    "    preserve_hierarchy=True\n",
    ")\n",
    "\n",
    "chunks = orchestrator.chunk_document(test_file, result, strategy=\"auto\", config=config)\n",
    "\n",
    "print(f\"Custom chunking created {len(chunks)} chunks\")\n",
    "print(f\"Config - Max: {config.max_chunk_size}, Min: {config.min_chunk_size}, Overlap: {config.overlap_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save with analysis metadata\n",
    "output_data = {\n",
    "    \"metadata\": {\n",
    "        \"file\": test_file,\n",
    "        \"processed_at\": datetime.now().isoformat(),\n",
    "        \"document_type\": result.type_name,\n",
    "        \"confidence\": result.confidence,\n",
    "        \"handler_used\": result.handler_used,\n",
    "        \"chunking_config\": {\n",
    "            \"strategy\": \"auto\",\n",
    "            \"max_chunk_size\": config.max_chunk_size,\n",
    "            \"min_chunk_size\": config.min_chunk_size,\n",
    "            \"overlap_size\": config.overlap_size,\n",
    "            \"preserve_hierarchy\": config.preserve_hierarchy\n",
    "        }\n",
    "    },\n",
    "    \"analysis\": {\n",
    "        \"ai_use_cases\": result.ai_use_cases,\n",
    "        \"key_findings\": result.key_findings,\n",
    "        \"quality_metrics\": result.quality_metrics\n",
    "    },\n",
    "    \"chunks\": [\n",
    "        {\n",
    "            \"chunk_id\": chunk.chunk_id,\n",
    "            \"content\": chunk.content,\n",
    "            \"element_path\": chunk.element_path,\n",
    "            \"start_line\": chunk.start_line,\n",
    "            \"end_line\": chunk.end_line,\n",
    "            \"elements_included\": chunk.elements_included,\n",
    "            \"metadata\": chunk.metadata,\n",
    "            \"token_estimate\": chunk.token_estimate\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"analysis_and_chunks.json\", \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved complete analysis with {len(chunks)} chunks to analysis_and_chunks.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All documentation examples have been tested! The framework successfully:\n",
    "\n",
    "1. ‚úÖ Analyzed the KML document and detected its type\n",
    "2. ‚úÖ Generated schema information\n",
    "3. ‚úÖ Created optimized chunks using different strategies\n",
    "4. ‚úÖ Exported all data to JSON format\n",
    "5. ‚úÖ Worked with custom configurations\n",
    "\n",
    "Check the generated JSON files to see the structured output!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}