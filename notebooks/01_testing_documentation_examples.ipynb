{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML Analysis Framework - Documentation Testing\n",
    "\n",
    "This notebook contains all the examples from the README documentation, allowing you to test them interactively and verify they work correctly.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "XML Analysis Framework version: 1.2.12\n"
     ]
    }
   ],
   "source": [
    "# Install the XML analysis framework\n",
    "%pip install xml-analysis-framework==1.2.12 --upgrade -q --force-reinstall --no-cache-dir\n",
    "\n",
    "# Import required modules\n",
    "import xml_analysis_framework as xaf\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"XML Analysis Framework version: {xaf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data\n",
    "\n",
    "We'll use a synthetic KML file for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test file found: data/mapbox-example.kml\n",
      "File size: 1.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Path to our test file\n",
    "test_file = \"data/mapbox-example.kml\"\n",
    "\n",
    "# Verify the file exists\n",
    "if Path(test_file).exists():\n",
    "    print(f\"‚úÖ Test file found: {test_file}\")\n",
    "    print(f\"File size: {Path(test_file).stat().st_size / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(f\"‚ùå Test file not found: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple API Examples\n",
    "\n",
    "Testing the simple API from the README:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document type: KML Geographic Data\n",
      "Handler used: KMLHandler\n",
      "Confidence: 0.95\n"
     ]
    }
   ],
   "source": [
    "# üéØ One-line analysis with specialized handlers\n",
    "result = xaf.analyze(test_file)\n",
    "print(f\"Document type: {result['document_type'].type_name}\")\n",
    "print(f\"Handler used: {result['handler_used']}\")\n",
    "print(f\"Confidence: {result['document_type'].confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 0.0 MB\n",
      "Using iterative parsing for large file: data/mapbox-example.kml\n",
      "Elements: 24, Depth: 6\n",
      "Root element: kml\n",
      "Namespaces: {'default': 'http://www.opengis.net/kml/2.2'}\n"
     ]
    }
   ],
   "source": [
    "# üìä Basic schema analysis  \n",
    "schema = xaf.analyze_schema(test_file)\n",
    "print(f\"Elements: {schema.total_elements}, Depth: {schema.max_depth}\")\n",
    "print(f\"Root element: {schema.root_element}\")\n",
    "print(f\"Namespaces: {schema.namespaces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 23 optimized chunks\n",
      "\n",
      "Chunk 1:\n",
      "  ID: chunk_0_ac4e63fc\n",
      "  Content length: 6616 chars\n",
      "  Element path: {http://www.opengis.net/kml/2.2}kml/{http://www.opengis.net/kml/2.2}Document/{http://www.opengis.net/kml/2.2}Placemark; {http://www.opengis.net/kml/2.2}kml; {http://www.opengis.net/kml/2.2}kml/{http://www.opengis.net/kml/2.2}Document\n",
      "  Token estimate: 271\n",
      "  Elements included: ['coordinates', 'kml', 'Point', 'outerBoundaryIs', 'name', 'Document', 'Placemark', 'LinearRing', 'Polygon']\n",
      "\n",
      "Chunk 2:\n",
      "  ID: chunk_1_4c6114a5\n",
      "  Content length: 4108 chars\n",
      "  Element path: {http://www.opengis.net/kml/2.2}kml/{http://www.opengis.net/kml/2.2}Document/{http://www.opengis.net/kml/2.2}Placemark/{http://www.opengis.net/kml/2.2}name; {http://www.opengis.net/kml/2.2}kml/{http://www.opengis.net/kml/2.2}Document/{http://www.opengis.net/kml/2.2}Placemark; {http://www.opengis.net/kml/2.2}kml/{http://www.opengis.net/kml/2.2}Document/{http://www.opengis.net/kml/2.2}Placemark/{http://www.opengis.net/kml/2.2}Point\n",
      "  Token estimate: 154\n",
      "  Elements included: ['coordinates', 'Point', 'outerBoundaryIs', 'name', 'Placemark', 'LinearRing', 'Polygon']\n",
      "\n",
      "Chunk 3:\n",
      "  ID: chunk_2_821008e5\n",
      "  Content length: 3887 chars\n",
      "  Element path: {http://www.opengis.net/kml/2.2}kml/{http://www.opengis.net/kml/2.2}Document/{http://www.opengis.net/kml/2.2}Placemark/{http://www.opengis.net/kml/2.2}name; {http://www.opengis.net/kml/2.2}kml/{http://www.opengis.net/kml/2.2}Document/{http://www.opengis.net/kml/2.2}Placemark/{http://www.opengis.net/kml/2.2}Point/{http://www.opengis.net/kml/2.2}coordinates; {http://www.opengis.net/kml/2.2}kml/{http://www.opengis.net/kml/2.2}Document/{http://www.opengis.net/kml/2.2}Placemark/{http://www.opengis.net/kml/2.2}Point\n",
      "  Token estimate: 145\n",
      "  Elements included: ['coordinates', 'Point', 'outerBoundaryIs', 'name', 'Placemark', 'LinearRing', 'Polygon']\n"
     ]
    }
   ],
   "source": [
    "# ‚úÇÔ∏è Smart chunking for AI/ML\n",
    "chunks = xaf.chunk(test_file, strategy=\"auto\")\n",
    "print(f\"Created {len(chunks)} optimized chunks\")\n",
    "\n",
    "# Show details of first few chunks\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  ID: {chunk.chunk_id}\")\n",
    "    print(f\"  Content length: {len(chunk.content)} chars\")\n",
    "    print(f\"  Element path: {chunk.element_path}\")\n",
    "    print(f\"  Token estimate: {chunk.token_estimate}\")\n",
    "    print(f\"  Elements included: {chunk.elements_included}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 23 chunks to chunks_output.json\n"
     ]
    }
   ],
   "source": [
    "# üíæ Save chunks to JSON\n",
    "chunks_data = [\n",
    "    {\n",
    "        \"chunk_id\": chunk.chunk_id,\n",
    "        \"content\": chunk.content,\n",
    "        \"element_path\": chunk.element_path,\n",
    "        \"start_line\": chunk.start_line,\n",
    "        \"end_line\": chunk.end_line,\n",
    "        \"elements_included\": chunk.elements_included,\n",
    "        \"metadata\": chunk.metadata,\n",
    "        \"token_estimate\": chunk.token_estimate\n",
    "    }\n",
    "    for chunk in chunks\n",
    "]\n",
    "\n",
    "# Write to file\n",
    "with open(\"chunks_output.json\", \"w\") as f:\n",
    "    json.dump(chunks_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(chunks_data)} chunks to chunks_output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced API Examples\n",
    "\n",
    "Testing the advanced API with multiple chunking strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: KML Geographic Data (confidence: 0.95)\n",
      "AI use cases: 9\n",
      "AI use cases: ['Geospatial pattern recognition', 'Location clustering and classification', 'Route optimization and analysis', 'Geographic feature extraction', 'Spatial relationship discovery', 'Area and distance calculations', 'Terrain and elevation analysis', 'Geographic anomaly detection', 'Location-based recommendations']\n",
      "Quality metrics: {'completeness': 0.0, 'organization': 0.0, 'richness': 0.0, 'precision': 0.5, 'overall': 0.1}\n",
      "\n",
      "Key findings: {'structure': {'documents': 1, 'folders': 0, 'features': [{'type': 'Placemark', 'count': 5}], 'total_features': 5, 'max_depth': 6, 'has_schema': False, 'has_extended_data': False}, 'placemarks': [{'name': 'Portland', 'description': None, 'geometry_type': 'Point', 'style_url': None, 'visibility': True, 'has_extended_data': False, 'snippet': None}, {'name': 'Rio de Janeiro', 'description': None, 'geometry_type': 'Point', 'style_url': None, 'visibility': True, 'has_extended_data': False, 'snippet': None}, {'name': 'Istanbul', 'description': None, 'geometry_type': 'Point', 'style_url': None, 'visibility': True, 'has_extended_data': False, 'snippet': None}, {'name': 'Reykjavik', 'description': None, 'geometry_type': 'Point', 'style_url': None, 'visibility': True, 'has_extended_data': False, 'snippet': None}, {'name': 'Simple Polygon', 'description': None, 'geometry_type': 'Polygon', 'style_url': None, 'visibility': True, 'has_extended_data': False, 'snippet': None}], 'geometries': {'total': 5, 'points': 4, 'lines': 0, 'polygons': 1, 'multi_geometries': 0, 'models': 0, 'coordinate_count': 9, 'altitude_modes': {}}, 'styles': [], 'overlays': {'total': 0, 'ground_overlays': [], 'screen_overlays': [], 'photo_overlays': []}, 'network_links': [], 'tours': {'count': 0, 'tour_info': []}, 'data_quality': {'has_names': 0, 'has_descriptions': 0, 'has_coordinates': 0, 'uses_styles': 0, 'organized_folders': False, 'uses_schemas': False, 'coordinate_precision': 'unknown'}}\n",
      "Structured data keys: ['geographic_bounds', 'feature_collection', 'style_definitions', 'metadata', 'coordinate_systems']\n"
     ]
    }
   ],
   "source": [
    "# Enhanced analysis with full results\n",
    "analysis = xaf.analyze_enhanced(test_file)\n",
    "\n",
    "print(f\"Type: {analysis.type_name} (confidence: {analysis.confidence:.2f})\")\n",
    "print(f\"AI use cases: {len(analysis.ai_use_cases)}\")\n",
    "print(f\"AI use cases: {analysis.ai_use_cases}\")\n",
    "\n",
    "if analysis.quality_metrics:\n",
    "    print(f\"Quality metrics: {analysis.quality_metrics}\")\n",
    "else:\n",
    "    print(\"Quality metrics: Not available\")\n",
    "\n",
    "print(f\"\\nKey findings: {analysis.key_findings}\")\n",
    "print(f\"Structured data keys: {list(analysis.structured_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical chunks: 0\n",
      "Sliding window chunks: 23\n",
      "Content-aware chunks: 24\n"
     ]
    }
   ],
   "source": [
    "# Different chunking strategies\n",
    "hierarchical_chunks = xaf.chunk(test_file, strategy=\"hierarchical\")\n",
    "sliding_chunks = xaf.chunk(test_file, strategy=\"sliding_window\") \n",
    "content_chunks = xaf.chunk(test_file, strategy=\"content_aware\")\n",
    "\n",
    "print(f\"Hierarchical chunks: {len(hierarchical_chunks)}\")\n",
    "print(f\"Sliding window chunks: {len(sliding_chunks)}\")\n",
    "print(f\"Content-aware chunks: {len(content_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 hierarchical chunks:\n"
     ]
    }
   ],
   "source": [
    "# Process chunks\n",
    "print(\"First 3 hierarchical chunks:\")\n",
    "for chunk in hierarchical_chunks[:3]:\n",
    "    print(f\"Chunk {chunk.chunk_id}: {len(chunk.content)} chars\")\n",
    "    print(f\"Path: {chunk.element_path}, Elements: {len(chunk.elements_included)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0 chunks to chunks_hierarchical.json\n",
      "Saved 23 chunks to chunks_sliding_window.json\n",
      "Saved 24 chunks to chunks_content_aware.json\n"
     ]
    }
   ],
   "source": [
    "# üíæ Save different chunking strategies to separate files\n",
    "# Helper function to convert chunk to dict\n",
    "def chunk_to_dict(chunk):\n",
    "    return {\n",
    "        \"chunk_id\": chunk.chunk_id,\n",
    "        \"content\": chunk.content,\n",
    "        \"element_path\": chunk.element_path,\n",
    "        \"start_line\": chunk.start_line,\n",
    "        \"end_line\": chunk.end_line,\n",
    "        \"elements_included\": chunk.elements_included,\n",
    "        \"metadata\": chunk.metadata,\n",
    "        \"token_estimate\": chunk.token_estimate\n",
    "    }\n",
    "\n",
    "# Save each strategy's results\n",
    "strategies = {\n",
    "    \"hierarchical\": hierarchical_chunks,\n",
    "    \"sliding_window\": sliding_chunks,\n",
    "    \"content_aware\": content_chunks\n",
    "}\n",
    "\n",
    "for strategy_name, chunks in strategies.items():\n",
    "    chunks_data = [chunk_to_dict(chunk) for chunk in chunks]\n",
    "    \n",
    "    with open(f\"chunks_{strategy_name}.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"strategy\": strategy_name,\n",
    "            \"total_chunks\": len(chunks_data),\n",
    "            \"chunks\": chunks_data\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(chunks_data)} chunks to chunks_{strategy_name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert API Examples\n",
    "\n",
    "Testing the expert API with direct class access and custom configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis result type: <class 'xml_analysis_framework.base.SpecializedAnalysis'>\n",
      "Document type: KML Geographic Data\n",
      "Confidence: 0.95\n",
      "Handler used: KMLHandler\n"
     ]
    }
   ],
   "source": [
    "# For advanced customization, use the classes directly\n",
    "from xml_analysis_framework import XMLDocumentAnalyzer, ChunkingOrchestrator\n",
    "\n",
    "analyzer = XMLDocumentAnalyzer(max_file_size_mb=500)\n",
    "orchestrator = ChunkingOrchestrator(max_file_size_mb=1000)\n",
    "\n",
    "# Custom analysis\n",
    "result = analyzer.analyze_document(test_file)\n",
    "\n",
    "print(f\"Analysis result type: {type(result)}\")\n",
    "print(f\"Document type: {result.type_name}\")\n",
    "print(f\"Confidence: {result.confidence}\")\n",
    "print(f\"Handler used: {result.handler_used}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom chunking created 23 chunks\n",
      "Config - Max: 2000, Min: 300, Overlap: 150\n"
     ]
    }
   ],
   "source": [
    "# Custom chunking with config\n",
    "from xml_analysis_framework.core.chunking import ChunkingConfig\n",
    "\n",
    "config = ChunkingConfig(\n",
    "    max_chunk_size=2000,\n",
    "    min_chunk_size=300,\n",
    "    overlap_size=150,\n",
    "    preserve_hierarchy=True\n",
    ")\n",
    "\n",
    "chunks = orchestrator.chunk_document(test_file, result, strategy=\"auto\", config=config)\n",
    "\n",
    "print(f\"Custom chunking created {len(chunks)} chunks\")\n",
    "print(f\"Config - Max: {config.max_chunk_size}, Min: {config.min_chunk_size}, Overlap: {config.overlap_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved complete analysis with 23 chunks to analysis_and_chunks.json\n"
     ]
    }
   ],
   "source": [
    "# üíæ Save with analysis metadata\n",
    "output_data = {\n",
    "    \"metadata\": {\n",
    "        \"file\": test_file,\n",
    "        \"processed_at\": datetime.now().isoformat(),\n",
    "        \"document_type\": result.type_name,\n",
    "        \"confidence\": result.confidence,\n",
    "        \"handler_used\": result.handler_used,\n",
    "        \"chunking_config\": {\n",
    "            \"strategy\": \"auto\",\n",
    "            \"max_chunk_size\": config.max_chunk_size,\n",
    "            \"min_chunk_size\": config.min_chunk_size,\n",
    "            \"overlap_size\": config.overlap_size,\n",
    "            \"preserve_hierarchy\": config.preserve_hierarchy\n",
    "        }\n",
    "    },\n",
    "    \"analysis\": {\n",
    "        \"ai_use_cases\": result.ai_use_cases,\n",
    "        \"key_findings\": result.key_findings,\n",
    "        \"quality_metrics\": result.quality_metrics\n",
    "    },\n",
    "    \"chunks\": [\n",
    "        {\n",
    "            \"chunk_id\": chunk.chunk_id,\n",
    "            \"content\": chunk.content,\n",
    "            \"element_path\": chunk.element_path,\n",
    "            \"start_line\": chunk.start_line,\n",
    "            \"end_line\": chunk.end_line,\n",
    "            \"elements_included\": chunk.elements_included,\n",
    "            \"metadata\": chunk.metadata,\n",
    "            \"token_estimate\": chunk.token_estimate\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"analysis_and_chunks.json\", \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved complete analysis with {len(chunks)} chunks to analysis_and_chunks.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All documentation examples have been tested! The framework successfully:\n",
    "\n",
    "1. ‚úÖ Analyzed the KML document and detected its type\n",
    "2. ‚úÖ Generated schema information\n",
    "3. ‚úÖ Created optimized chunks using different strategies\n",
    "4. ‚úÖ Exported all data to JSON format\n",
    "5. ‚úÖ Worked with custom configurations\n",
    "\n",
    "Check the generated JSON files to see the structured output!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
