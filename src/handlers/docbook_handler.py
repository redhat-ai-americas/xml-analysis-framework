#!/usr/bin/env python3
"""
DocBook Handler

Analyzes DocBook XML documentation files for structure analysis, content organization,
technical documentation quality assessment, and documentation generation workflows.
"""

import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import sys
import os

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.analyzer import XMLHandler, DocumentTypeInfo, SpecializedAnalysis


class DocBookHandler(XMLHandler):
    """Handler for DocBook XML documentation files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for DocBook elements
        docbook_roots = ['book', 'article', 'chapter', 'section', 'para']
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        if tag in docbook_roots:
            return True, 0.8
        
        # Check for DocBook namespace
        if any('docbook.org' in uri for uri in namespaces.values()):
            return True, 1.0
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        # Detect DocBook version
        version = self._detect_docbook_version(root, namespaces)
        
        metadata = {
            "framework": "DocBook",
            "category": "technical_documentation",
            "document_type": tag,
            "has_chapters": len(root.findall('.//chapter')) > 0,
            "has_sections": len(root.findall('.//section')) > 0,
            "element_count": len(list(root.iter()))
        }
        
        return DocumentTypeInfo(
            type_name="DocBook Documentation",
            confidence=0.9,
            version=version,
            metadata=metadata
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'docbook_info': {
                'document_type': root.tag.split('}')[-1] if '}' in root.tag else root.tag,
                'version': self._detect_docbook_version(root, {}),
                'namespaces': self._extract_namespace_info(root)
            },
            'structure': self._analyze_document_structure(root),
            'metadata': self._extract_metadata(root),
            'content_stats': self._analyze_content(root),
            'media': self._find_media_references(root),
            'cross_references': self._find_cross_references(root),
            'accessibility': self._analyze_accessibility_features(root),
            'quality_indicators': self._analyze_quality_indicators(root),
            'localization': self._analyze_localization_features(root),
            'publishing': self._analyze_publishing_features(root)
        }
        
        recommendations = [
            "Extract for documentation search system integration",
            "Generate multiple output formats (HTML, PDF, EPUB)",
            "Check for broken cross-references and validate links",
            "Analyze readability and completeness metrics",
            "Validate accessibility compliance (WCAG guidelines)",
            "Review document structure for logical organization",
            "Check media references for missing files",
            "Optimize for multi-format publishing workflows",
            "Validate DocBook schema compliance"
        ]
        
        ai_use_cases = [
            "Documentation quality analysis and improvement suggestions",
            "Automatic summary and abstract generation",
            "Technical content extraction and indexing",
            "Glossary and index generation from content",
            "Documentation translation and localization support",
            "Cross-reference validation and link checking",
            "Content accessibility analysis and enhancement",
            "Multi-format publishing optimization",
            "Document structure analysis and reorganization suggestions",
            "Technical writing style analysis and improvement"
        ]
        
        data_inventory = {
            'chapters': len(findings['structure']['chapters']),
            'sections': findings['structure']['total_sections'],
            'paragraphs': findings['content_stats']['paragraphs'],
            'media_items': len(findings['media']['media_references']),
            'cross_references': len(findings['cross_references']['internal_references']),
            'code_examples': len(findings['content_stats']['code_examples']),
            'tables': findings['content_stats']['tables'],
            'lists': findings['content_stats']['lists']
        }
        
        return SpecializedAnalysis(
            document_type=f"DocBook {findings['docbook_info']['document_type'].title()}",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=data_inventory,
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_documentation_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'document_metadata': {
                'title': self._extract_title(root),
                'type': root.tag.split('}')[-1] if '}' in root.tag else root.tag,
                'version': self._detect_docbook_version(root, {}),
                'language': root.get('lang') or root.get('{http://www.w3.org/XML/1998/namespace}lang')
            },
            'content_summary': self._extract_content_summary(root),
            'structural_summary': self._extract_structural_summary(root),
            'technical_summary': self._extract_technical_summary(root)
        }
    
    def _detect_docbook_version(self, root: ET.Element, namespaces: Dict[str, str]) -> str:
        """Detect DocBook version from namespace or DTD"""
        # Check namespace for version
        for uri in namespaces.values():
            if 'docbook.org' in uri:
                if '/5.0/' in uri:
                    return "5.0"
                elif '/4.5/' in uri:
                    return "4.5"
                elif '/4.4/' in uri:
                    return "4.4"
                elif '/4.3/' in uri:
                    return "4.3"
        
        # Check root tag namespace
        if '}' in root.tag:
            namespace = root.tag.split('}')[0][1:]
            if 'docbook.org' in namespace:
                if '/5.0/' in namespace:
                    return "5.0"
                elif '/4.' in namespace:
                    return "4.x"
        
        # Default assumption
        return "5.0"
    
    def _extract_namespace_info(self, root: ET.Element) -> Dict[str, Any]:
        """Extract namespace information"""
        namespaces = {}
        for key, value in root.attrib.items():
            if key.startswith('xmlns'):
                prefix = key.split(':', 1)[1] if ':' in key else 'default'
                namespaces[prefix] = value
        
        return {
            'declared_namespaces': namespaces,
            'docbook_namespace': any('docbook.org' in uri for uri in namespaces.values()),
            'xlink_namespace': 'xlink' in namespaces
        }
    
    def _analyze_document_structure(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze document structure and hierarchy"""
        # Handle namespaced elements properly
        ns_prefix = '{http://docbook.org/ns/docbook}' if '}' in root.tag else ''
        
        structure = {
            'document_type': root.tag.split('}')[-1] if '}' in root.tag else root.tag,
            'chapters': [],
            'total_sections': 0,
            'max_depth': 0,
            'parts': [],
            'appendices': [],
            'hierarchy_analysis': {}
        }
        
        # Analyze parts
        for part in root.findall(f'.//{ns_prefix}part'):
            title_elem = part.find(f'.//{ns_prefix}title')
            structure['parts'].append({
                'title': title_elem.text if title_elem is not None else 'Untitled Part',
                'chapters': len(part.findall(f'.//{ns_prefix}chapter')),
                'sections': len(part.findall(f'.//{ns_prefix}section'))
            })
        
        # Analyze chapters
        for chapter in root.findall(f'.//{ns_prefix}chapter'):
            title_elem = chapter.find(f'.//{ns_prefix}title')
            chapter_info = {
                'title': title_elem.text if title_elem is not None else 'Untitled Chapter',
                'sections': len(chapter.findall(f'.//{ns_prefix}section')),
                'subsections': len(chapter.findall(f'.//{ns_prefix}section//{ns_prefix}section')),
                'id': chapter.get('id') or chapter.get('{http://www.w3.org/XML/1998/namespace}id')
            }
            structure['chapters'].append(chapter_info)
        
        # Analyze appendices
        for appendix in root.findall(f'.//{ns_prefix}appendix'):
            title_elem = appendix.find(f'.//{ns_prefix}title')
            structure['appendices'].append({
                'title': title_elem.text if title_elem is not None else 'Untitled Appendix',
                'sections': len(appendix.findall(f'.//{ns_prefix}section'))
            })
        
        # Count all sections
        structure['total_sections'] = len(root.findall(f'.//{ns_prefix}section'))
        
        # Calculate hierarchy depth
        structure['max_depth'] = self._calculate_max_depth(root)
        
        # Analyze hierarchy consistency
        structure['hierarchy_analysis'] = self._analyze_hierarchy_consistency(root)
        
        return structure
    
    def _extract_metadata(self, root: ET.Element) -> Dict[str, Any]:
        """Extract document metadata from info elements"""
        # DocBook 5.x uses 'info', 4.x uses specific info elements
        info = (root.find('.//info') or 
                root.find('.//bookinfo') or 
                root.find('.//articleinfo') or
                root.find('.//chapterinfo'))
        
        if info is None:
            return {
                'has_metadata': False,
                'title': self._extract_title(root),
                'extracted_from': 'root_title_only'
            }
        
        metadata = {
            'has_metadata': True,
            'title': getattr(info.find('.//title'), 'text', None),
            'subtitle': getattr(info.find('.//subtitle'), 'text', None),
            'author': self._extract_author_info(info),
            'date': getattr(info.find('.//date'), 'text', None),
            'publisher': getattr(info.find('.//publisher//publishername'), 'text', None),
            'copyright': self._extract_copyright_info(info),
            'abstract': self._extract_abstract(info),
            'keywords': self._extract_keywords(info),
            'revision_history': self._extract_revision_history(info)
        }
        
        return metadata
    
    def _analyze_content(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze content statistics and types"""
        # Handle namespaced elements properly
        ns_prefix = '{http://docbook.org/ns/docbook}' if '}' in root.tag else ''
        
        content_stats = {
            'paragraphs': len(root.findall(f'.//{ns_prefix}para')),
            'lists': len(root.findall(f'.//{ns_prefix}itemizedlist')) + len(root.findall(f'.//{ns_prefix}orderedlist')),
            'tables': len(root.findall(f'.//{ns_prefix}table')) + len(root.findall(f'.//{ns_prefix}informaltable')),
            'figures': len(root.findall(f'.//{ns_prefix}figure')),
            'examples': len(root.findall(f'.//{ns_prefix}example')),
            'equations': len(root.findall(f'.//{ns_prefix}equation')) + len(root.findall(f'.//{ns_prefix}informalequation')),
            'code_examples': self._count_code_examples(root),
            'admonitions': {
                'notes': len(root.findall(f'.//{ns_prefix}note')),
                'warnings': len(root.findall(f'.//{ns_prefix}warning')),
                'cautions': len(root.findall(f'.//{ns_prefix}caution')),
                'important': len(root.findall(f'.//{ns_prefix}important')),
                'tips': len(root.findall(f'.//{ns_prefix}tip'))
            },
            'footnotes': len(root.findall(f'.//{ns_prefix}footnote')),
            'bibliographic_entries': len(root.findall(f'.//{ns_prefix}biblioentry'))
        }
        
        # Add content density analysis
        total_elements = len(list(root.iter()))
        content_stats['content_density'] = {
            'paragraph_density': content_stats['paragraphs'] / total_elements if total_elements > 0 else 0,
            'example_density': len(content_stats['code_examples']) / total_elements if total_elements > 0 else 0,
            'media_density': (content_stats['figures'] + content_stats['tables']) / total_elements if total_elements > 0 else 0
        }
        
        return content_stats
    
    def _find_media_references(self, root: ET.Element) -> Dict[str, Any]:
        """Find and analyze media references"""
        media_info = {
            'media_references': [],
            'media_types': {},
            'broken_references_check': [],
            'accessibility_analysis': {}
        }
        
        # Find image references
        for elem in root.findall('.//imagedata'):
            media_ref = {
                'type': 'image',
                'fileref': elem.get('fileref'),
                'format': elem.get('format'),
                'width': elem.get('width'),
                'depth': elem.get('depth'),
                'scale': elem.get('scale'),
                'align': elem.get('align'),
                'parent_element': elem.getparent().tag if hasattr(elem, 'getparent') else 'unknown'
            }
            media_info['media_references'].append(media_ref)
            
            # Count media types
            format_type = elem.get('format', 'unknown')
            media_info['media_types'][format_type] = media_info['media_types'].get(format_type, 0) + 1
        
        # Find video/audio references
        for elem in root.findall('.//videodata'):
            media_info['media_references'].append({
                'type': 'video',
                'fileref': elem.get('fileref'),
                'format': elem.get('format')
            })
        
        for elem in root.findall('.//audiodata'):
            media_info['media_references'].append({
                'type': 'audio',
                'fileref': elem.get('fileref'),
                'format': elem.get('format')
            })
        
        # Analyze accessibility features for media
        media_info['accessibility_analysis'] = self._analyze_media_accessibility(root)
        
        return media_info
    
    def _find_cross_references(self, root: ET.Element) -> Dict[str, Any]:
        """Find and analyze cross-references"""
        xref_info = {
            'internal_references': [],
            'external_references': [],
            'link_analysis': {},
            'reference_targets': set(),
            'orphaned_references': []
        }
        
        # Find all elements with IDs (potential targets)
        for elem in root.iter():
            elem_id = elem.get('id') or elem.get('{http://www.w3.org/XML/1998/namespace}id')
            if elem_id:
                xref_info['reference_targets'].add(elem_id)
        
        # Find xref elements
        for xref in root.findall('.//xref'):
            linkend = xref.get('linkend')
            if linkend:
                xref_info['internal_references'].append({
                    'linkend': linkend,
                    'exists': linkend in xref_info['reference_targets'],
                    'element': 'xref'
                })
        
        # Find link elements
        for link in root.findall('.//link'):
            linkend = link.get('linkend')
            if linkend:
                xref_info['internal_references'].append({
                    'linkend': linkend,
                    'exists': linkend in xref_info['reference_targets'],
                    'element': 'link'
                })
        
        # Find external links
        for ulink in root.findall('.//ulink'):
            url = ulink.get('url')
            if url:
                xref_info['external_references'].append({
                    'url': url,
                    'text': ulink.text,
                    'element': 'ulink'
                })
        
        # Analyze broken references
        for ref in xref_info['internal_references']:
            if not ref['exists']:
                xref_info['orphaned_references'].append(ref['linkend'])
        
        # Generate link analysis
        xref_info['link_analysis'] = {
            'total_internal': len(xref_info['internal_references']),
            'total_external': len(xref_info['external_references']),
            'broken_internal': len(xref_info['orphaned_references']),
            'reference_density': len(xref_info['internal_references']) / max(len(list(root.iter())), 1)
        }
        
        return xref_info
    
    def _analyze_accessibility_features(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze accessibility features"""
        accessibility = {
            'alt_text_coverage': 0,
            'table_accessibility': {},
            'structure_accessibility': {},
            'language_support': {},
            'accessibility_score': 0.0,
            'recommendations': []
        }
        
        # Check alt text for images
        images = root.findall('.//imagedata')
        images_with_alt = 0
        for img in images:
            # Check for alt attribute or textobject
            parent_figure = img.getparent() if hasattr(img, 'getparent') else None
            if parent_figure and parent_figure.find('.//textobject'):
                images_with_alt += 1
        
        if images:
            accessibility['alt_text_coverage'] = images_with_alt / len(images)
        
        # Check table accessibility
        tables = root.findall('.//table') + root.findall('.//informaltable')
        tables_with_headers = sum(1 for table in tables if table.find('.//thead'))
        accessibility['table_accessibility'] = {
            'total_tables': len(tables),
            'tables_with_headers': tables_with_headers,
            'header_coverage': tables_with_headers / len(tables) if tables else 0
        }
        
        # Check structural accessibility
        accessibility['structure_accessibility'] = {
            'has_title': root.find('.//title') is not None,
            'has_toc': root.find('.//toc') is not None,
            'section_hierarchy': len(root.findall('.//section')) > 0
        }
        
        # Calculate accessibility score
        score = 0
        if accessibility['alt_text_coverage'] > 0.8:
            score += 0.3
        if accessibility['table_accessibility']['header_coverage'] > 0.8:
            score += 0.3
        if accessibility['structure_accessibility']['has_title']:
            score += 0.2
        if accessibility['structure_accessibility']['section_hierarchy']:
            score += 0.2
        
        accessibility['accessibility_score'] = score
        
        # Generate recommendations
        if accessibility['alt_text_coverage'] < 0.8:
            accessibility['recommendations'].append('Add alt text or textobject for images')
        if accessibility['table_accessibility']['header_coverage'] < 0.8:
            accessibility['recommendations'].append('Add proper table headers for accessibility')
        if not accessibility['structure_accessibility']['has_toc']:
            accessibility['recommendations'].append('Consider adding table of contents for navigation')
        
        return accessibility
    
    def _analyze_quality_indicators(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze documentation quality indicators"""
        quality = {
            'completeness_indicators': {},
            'consistency_checks': {},
            'best_practices': {},
            'quality_score': 0.0
        }
        
        # Completeness indicators
        has_abstract = root.find('.//abstract') is not None
        has_examples = len(root.findall('.//example')) > 0
        has_index = root.find('.//index') is not None
        has_bibliography = root.find('.//bibliography') is not None
        
        quality['completeness_indicators'] = {
            'has_abstract': has_abstract,
            'has_examples': has_examples,
            'has_index': has_index,
            'has_bibliography': has_bibliography,
            'completeness_score': sum([has_abstract, has_examples, has_index, has_bibliography]) / 4
        }
        
        # Consistency checks
        sections_with_titles = len([s for s in root.findall('.//section') if s.find('.//title') is not None])
        total_sections = len(root.findall('.//section'))
        
        quality['consistency_checks'] = {
            'section_title_consistency': sections_with_titles / total_sections if total_sections > 0 else 1.0,
            'id_consistency': self._check_id_consistency(root)
        }
        
        # Best practices
        quality['best_practices'] = {
            'uses_semantic_markup': self._check_semantic_markup(root),
            'proper_nesting': self._check_proper_nesting(root),
            'metadata_present': root.find('.//info') is not None
        }
        
        # Calculate overall quality score
        quality['quality_score'] = (
            quality['completeness_indicators']['completeness_score'] * 0.4 +
            quality['consistency_checks']['section_title_consistency'] * 0.3 +
            sum(quality['best_practices'].values()) / len(quality['best_practices']) * 0.3
        )
        
        return quality
    
    def _analyze_localization_features(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze localization and internationalization features"""
        localization = {
            'language_info': {},
            'translatable_content': {},
            'localization_support': {}
        }
        
        # Check language attributes
        lang = root.get('lang') or root.get('{http://www.w3.org/XML/1998/namespace}lang')
        localization['language_info'] = {
            'primary_language': lang,
            'has_language_attribute': lang is not None,
            'multiple_languages': len(set(elem.get('lang') or elem.get('{http://www.w3.org/XML/1998/namespace}lang') 
                                          for elem in root.iter() if elem.get('lang') or elem.get('{http://www.w3.org/XML/1998/namespace}lang'))) > 1
        }
        
        # Analyze translatable content
        text_elements = len([elem for elem in root.iter() if elem.text and elem.text.strip()])
        localization['translatable_content'] = {
            'text_elements': text_elements,
            'estimated_word_count': self._estimate_word_count(root)
        }
        
        return localization
    
    def _analyze_publishing_features(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze publishing and output generation features"""
        publishing = {
            'output_hints': {},
            'formatting_elements': {},
            'publishing_metadata': {}
        }
        
        # Check for output-specific elements
        publishing['output_hints'] = {
            'has_page_breaks': len(root.findall('.//pagebreak')) > 0,
            'has_index_terms': len(root.findall('.//indexterm')) > 0,
            'has_processing_instructions': len([elem for elem in root.iter() if elem.tag.startswith('<?')]) > 0
        }
        
        # Check formatting elements
        publishing['formatting_elements'] = {
            'emphasis_elements': len(root.findall('.//emphasis')),
            'literal_elements': len(root.findall('.//literal')),
            'subscript_superscript': len(root.findall('.//subscript')) + len(root.findall('.//superscript'))
        }
        
        return publishing
    
    def _count_code_examples(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Count and analyze code examples"""
        code_examples = []
        
        for example in root.findall('.//programlisting'):
            code_info = {
                'language': example.get('language', 'unknown'),
                'linenumbering': example.get('linenumbering') == 'numbered',
                'length': len(example.text) if example.text else 0,
                'has_title': example.getparent() and example.getparent().find('.//title') is not None if hasattr(example, 'getparent') else False
            }
            code_examples.append(code_info)
        
        # Also check screen elements (command line examples)
        for screen in root.findall('.//screen'):
            code_examples.append({
                'language': 'shell',
                'type': 'screen_output',
                'length': len(screen.text) if screen.text else 0
            })
        
        return code_examples[:20]  # Limit to first 20 examples
    
    def _calculate_max_depth(self, root: ET.Element, depth: int = 0) -> int:
        """Calculate maximum nesting depth"""
        if not list(root):
            return depth
        return max(self._calculate_max_depth(child, depth + 1) for child in root)
    
    def _analyze_hierarchy_consistency(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze document hierarchy consistency"""
        hierarchy = {
            'consistent_numbering': True,
            'proper_nesting': True,
            'missing_titles': 0
        }
        
        # Check for missing titles in structural elements
        structural_elements = ['chapter', 'section', 'appendix', 'part']
        for elem_type in structural_elements:
            elements = root.findall(f'.//{elem_type}')
            for elem in elements:
                if elem.find('.//title') is None:
                    hierarchy['missing_titles'] += 1
        
        return hierarchy
    
    def _extract_author_info(self, info: ET.Element) -> Dict[str, Any]:
        """Extract comprehensive author information"""
        authors = []
        
        for author in info.findall('.//author'):
            author_info = {}
            
            # Name components
            firstname = author.find('.//firstname')
            surname = author.find('.//surname')
            othername = author.find('.//othername')
            
            if firstname is not None and surname is not None:
                author_info['name'] = f"{firstname.text} {surname.text}"
                if othername is not None:
                    author_info['name'] = f"{firstname.text} {othername.text} {surname.text}"
            
            # Contact info
            email = author.find('.//email')
            if email is not None:
                author_info['email'] = email.text
            
            # Affiliation
            affiliation = author.find('.//affiliation')
            if affiliation is not None:
                orgname = affiliation.find('.//orgname')
                if orgname is not None:
                    author_info['organization'] = orgname.text
            
            authors.append(author_info)
        
        return authors if authors else None
    
    def _extract_copyright_info(self, info: ET.Element) -> Optional[Dict[str, Any]]:
        """Extract copyright information"""
        copyright_elem = info.find('.//copyright')
        if copyright_elem is None:
            return None
        
        copyright_info = {}
        
        year = copyright_elem.find('.//year')
        if year is not None:
            copyright_info['year'] = year.text
        
        holder = copyright_elem.find('.//holder')
        if holder is not None:
            copyright_info['holder'] = holder.text
        
        return copyright_info
    
    def _extract_abstract(self, info: ET.Element) -> Optional[str]:
        """Extract document abstract"""
        abstract = info.find('.//abstract')
        if abstract is not None:
            # Combine all paragraph text
            paras = abstract.findall('.//para')
            if paras:
                return ' '.join(para.text for para in paras if para.text)
            return abstract.text
        return None
    
    def _extract_keywords(self, info: ET.Element) -> List[str]:
        """Extract keywords"""
        keywords = []
        keywordset = info.find('.//keywordset')
        if keywordset is not None:
            for keyword in keywordset.findall('.//keyword'):
                if keyword.text:
                    keywords.append(keyword.text)
        return keywords
    
    def _extract_revision_history(self, info: ET.Element) -> List[Dict[str, Any]]:
        """Extract revision history"""
        revisions = []
        revhistory = info.find('.//revhistory')
        if revhistory is not None:
            for revision in revhistory.findall('.//revision'):
                rev_info = {}
                
                revnumber = revision.find('.//revnumber')
                if revnumber is not None:
                    rev_info['number'] = revnumber.text
                
                date = revision.find('.//date')
                if date is not None:
                    rev_info['date'] = date.text
                
                authorinitials = revision.find('.//authorinitials')
                if authorinitials is not None:
                    rev_info['author'] = authorinitials.text
                
                revremark = revision.find('.//revremark')
                if revremark is not None:
                    rev_info['remark'] = revremark.text
                
                revisions.append(rev_info)
        
        return revisions
    
    def _analyze_media_accessibility(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze media accessibility features"""
        accessibility = {
            'images_with_alt': 0,
            'total_images': 0,
            'media_descriptions': 0
        }
        
        for mediaobject in root.findall('.//mediaobject'):
            accessibility['total_images'] += 1
            
            # Check for textobject (alt text equivalent)
            if mediaobject.find('.//textobject'):
                accessibility['images_with_alt'] += 1
        
        return accessibility
    
    def _check_id_consistency(self, root: ET.Element) -> float:
        """Check consistency of ID attributes"""
        elements_with_ids = 0
        structural_elements = 0
        
        for elem in root.findall('.//chapter') + root.findall('.//section') + root.findall('.//appendix'):
            structural_elements += 1
            if elem.get('id') or elem.get('{http://www.w3.org/XML/1998/namespace}id'):
                elements_with_ids += 1
        
        return elements_with_ids / structural_elements if structural_elements > 0 else 1.0
    
    def _check_semantic_markup(self, root: ET.Element) -> bool:
        """Check for proper semantic markup usage"""
        semantic_elements = ['emphasis', 'literal', 'filename', 'command', 'option', 'replaceable']
        return any(len(root.findall(f'.//{elem}')) > 0 for elem in semantic_elements)
    
    def _check_proper_nesting(self, root: ET.Element) -> bool:
        """Check for proper element nesting"""
        # This is a simplified check - would need more comprehensive validation
        return True  # Assume proper nesting unless we detect issues
    
    def _estimate_word_count(self, root: ET.Element) -> int:
        """Estimate word count for translatable content"""
        word_count = 0
        for elem in root.iter():
            if elem.text and elem.text.strip():
                word_count += len(elem.text.strip().split())
        return word_count
    
    def _extract_title(self, root: ET.Element) -> str:
        """Extract document title"""
        # Handle namespaced elements properly
        ns_prefix = '{http://docbook.org/ns/docbook}' if '}' in root.tag else ''
        title = root.find(f'.//{ns_prefix}title')
        return title.text if title is not None else 'Untitled Document'
    
    def _extract_content_summary(self, root: ET.Element) -> Dict[str, Any]:
        """Extract content summary"""
        content = self._analyze_content(root)
        return {
            'total_paragraphs': content['paragraphs'],
            'content_types': {
                'narrative': content['paragraphs'],
                'examples': len(content['code_examples']),
                'tables': content['tables'],
                'figures': content['figures']
            },
            'estimated_reading_time': content['paragraphs'] * 0.5  # Rough estimate in minutes
        }
    
    def _extract_structural_summary(self, root: ET.Element) -> Dict[str, Any]:
        """Extract structural summary"""
        structure = self._analyze_document_structure(root)
        return {
            'document_type': structure['document_type'],
            'major_sections': len(structure['chapters']),
            'total_sections': structure['total_sections'],
            'max_nesting_depth': structure['max_depth'],
            'has_appendices': len(structure['appendices']) > 0
        }
    
    def _extract_technical_summary(self, root: ET.Element) -> Dict[str, Any]:
        """Extract technical summary"""
        return {
            'docbook_version': self._detect_docbook_version(root, {}),
            'namespace_count': len([k for k in root.attrib.keys() if k.startswith('xmlns')]),
            'cross_references': len(root.findall('.//xref')),
            'external_links': len(root.findall('.//ulink')),
            'media_objects': len(root.findall('.//mediaobject'))
        }
    
    def _assess_documentation_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        """Assess overall documentation quality"""
        
        # Content quality
        content = findings['content_stats']
        structure = findings['structure']
        
        content_score = 0.0
        if content['paragraphs'] > 10:  # Substantial content
            content_score += 0.3
        if len(content['code_examples']) > 0:  # Has examples
            content_score += 0.3
        if content['admonitions']['notes'] + content['admonitions']['warnings'] > 0:  # Has helpful notes
            content_score += 0.2
        if content['tables'] > 0 or content['figures'] > 0:  # Has visual aids
            content_score += 0.2
        
        # Structure quality
        structure_score = 0.0
        if structure['total_sections'] > 0:  # Well structured
            structure_score += 0.4
        if len(structure['chapters']) > 1:  # Multi-chapter document
            structure_score += 0.3
        if structure['max_depth'] <= 5:  # Reasonable depth
            structure_score += 0.3
        
        # Metadata quality
        metadata = findings['metadata']
        metadata_score = 0.0
        if metadata['has_metadata']:
            metadata_score += 0.5
        if metadata.get('author'):
            metadata_score += 0.3
        if metadata.get('abstract'):
            metadata_score += 0.2
        
        # Accessibility quality
        accessibility_score = findings['accessibility']['accessibility_score']
        
        # Cross-reference quality
        xref = findings['cross_references']
        xref_score = 1.0
        if xref['link_analysis']['total_internal'] > 0:
            broken_ratio = xref['link_analysis']['broken_internal'] / xref['link_analysis']['total_internal']
            xref_score = max(0.0, 1.0 - broken_ratio)
        
        return {
            "content_quality": content_score,
            "structure_quality": structure_score,
            "metadata_quality": metadata_score,
            "accessibility": accessibility_score,
            "reference_integrity": xref_score,
            "overall": (content_score + structure_score + metadata_score + accessibility_score + xref_score) / 5
        }